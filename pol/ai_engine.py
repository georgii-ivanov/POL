import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
import requests
import logging
import os
import time
import json
import hashlib
import math
from typing import Dict, List, Optional, Tuple, Any
from .models.revolutionary_ai import RevolutionaryAIModel, RevolutionaryAIConfig, SimpleTransformerModel
from .models.advanced_gpt import AdvancedGPTModel
from .data_acquisition import InternetDataAcquisitionEngine
from .crypto import CryptoManager
from .training_blockchain import TrainingBlockchain, TrainingEntry
import psutil
from contextlib import nullcontext
import asyncio

# Import GradScaler and autocast properly for different devices
try:
    from torch.amp import GradScaler, autocast
except ImportError:
    # Fallback for older PyTorch versions
    from torch.cuda.amp import GradScaler, autocast

logger = logging.getLogger(__name__)

class AITrainingEngine:
    """Revolutionary AI Training Engine with Blockchain-Protected Progress"""
    
    def __init__(self, config: Dict[str, Any], node_id: str = "default_node"):
        # Handle both dict and object configs properly
        if isinstance(config, dict):
            self.config = config
        else:
            # Convert config object to dict for compatibility
            self.config = {
                'model_type': getattr(config, 'model_type', 'revolutionary'),
                'vocab_size': getattr(config, 'vocab_size', 100000),  # Advanced tokenizer size
                'embed_dim': getattr(config, 'embed_dim', 768),
                'num_heads': getattr(config, 'num_heads', 12),
                'num_layers': getattr(config, 'num_layers', 12),
                'max_seq_length': getattr(config, 'max_seq_length', 1024),
                'batch_size': getattr(config, 'batch_size', 6),  # Optimized for 64GB M1 Max
                'learning_rate': getattr(config, 'learning_rate', 1e-4),  # Conservative learning rate for stability
                'weight_decay': getattr(config, 'weight_decay', 0.01),
                'num_experts': getattr(config, 'num_experts', 8),
                'consciousness_dim': getattr(config, 'consciousness_dim', 256),
                'quantum_coherence_layers': getattr(config, 'quantum_coherence_layers', 4),
                'checkpoint_dir': getattr(config, 'checkpoint_dir', './checkpoints'),
                'distributed_training': getattr(config, 'distributed_training', True),
                'model_sync_interval': getattr(config, 'model_sync_interval', 300),
                'max_batches_per_epoch': getattr(config, 'max_batches_per_epoch', 10),
                'load_pretrained_base': getattr(config, 'load_pretrained_base', True),  # Enable by default
                'force_pretrained_download': getattr(config, 'force_pretrained_download', False),
                'pretrained_model_name': getattr(config, 'pretrained_model_name', 'auto'),  # Auto-select best model
                'use_pretrained_data_for_training': getattr(config, 'use_pretrained_data_for_training', True),
                'pretrained_model_priority': getattr(config, 'pretrained_model_priority', [
                    # State-of-the-art open-source models (in priority order)
                    'microsoft/DialoGPT-large',           # Conversational AI
                    'EleutherAI/gpt-neo-2.7B',           # General language model
                    'EleutherAI/gpt-j-6B',               # Advanced language model
                    'microsoft/DialoGPT-medium',          # Fallback conversational
                    'gpt2-xl',                           # Classic but reliable
                    'distilgpt2',                        # Lightweight fallback
                ]),
                'adapt_architecture_to_pretrained': getattr(config, 'adapt_architecture_to_pretrained', True),
                'extract_pretrained_training_data': getattr(config, 'extract_pretrained_training_data', True)
            }
        
        # Add helper method for safe config access
        def get_config_value(key: str, default=None):
            if isinstance(self.config, dict):
                return self.config.get(key, default)
            else:
                return getattr(self.config, key, default)
        self._get_config = get_config_value
        
        self.node_id = node_id
        
        # Revolutionary blockchain-based training protection!
        self.training_blockchain = TrainingBlockchain(
            chain_dir=os.path.join(self.config.get('checkpoint_dir', './checkpoints'), 'training_chain')
        )
        
        # Initialize training state from blockchain
        self._initialize_from_blockchain()
        
        # Move resource detection here to ensure it's available
        # Comprehensive resource detection and configuration
        self.compute_resources = self._detect_compute_resources()
        self.device = torch.device(self.compute_resources['primary_device'])
        
        # Update configuration based on detected resources
        self.config['batch_size'] = self.compute_resources['optimal_batch_size']
        self.config['max_seq_length'] = min(
            self.config.get('max_seq_length', 1024),
            self.compute_resources['max_sequence_length']
        )
        
        logger.info(f"ðŸ”¥ Using optimized device: {self.device}")
        logger.info(f"ðŸ“± Device capabilities: {self.compute_resources['gpus'][0]['name'] if self.compute_resources['gpus'] else 'CPU Only'}")
        
        # Apply device-specific optimizations for 90% resource usage
        if self.device.type == 'cpu':
            # Set optimal thread count for 90% CPU utilization
            cpu_threads = max(1, int(self.compute_resources['cpu_cores'] * 0.9))
            torch.set_num_threads(cpu_threads)
            torch.set_num_interop_threads(max(1, cpu_threads // 2))
            logger.info(f"ðŸ”§ CPU Optimization: Using {cpu_threads} threads ({self.compute_resources['cpu_cores']} cores available)")
            
        elif self.device.type == 'cuda':
            # Enable CUDA optimizations
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            logger.info("ðŸ”¥ CUDA optimizations enabled (benchmark mode)")
            
        elif self.device.type == 'mps':
            # MPS-specific optimizations
            logger.info("ðŸŽ Apple Metal optimizations active")
        
        # Initialize all required attributes early
        self.model_type = self.config.get('model_type', 'revolutionary')
        self.vocab_size = self.config.get('vocab_size', 100000)
        self.embed_dim = self.config.get('embed_dim', 768)
        self.num_heads = self.config.get('num_heads', 12)
        self.num_layers = self.config.get('num_layers', 12)
        self.max_seq_length = self.config.get('max_seq_length', 1024)
        
        # Pretrained model configuration
        self.load_pretrained_base = self.config.get('load_pretrained_base', True)  # Enable by default
        self.force_pretrained_download = self.config.get('force_pretrained_download', False)
        self.pretrained_model_name = self.config.get('pretrained_model_name', 'auto')  # Auto-select best model
        
        # Advanced pretrained model options
        self.use_pretrained_data_for_training = self.config.get('use_pretrained_data_for_training', True)
        self.pretrained_model_priority = self.config.get('pretrained_model_priority', [
            # State-of-the-art open-source models (in priority order)
            'microsoft/DialoGPT-large',           # Conversational AI
            'EleutherAI/gpt-neo-2.7B',           # General language model
            'EleutherAI/gpt-j-6B',               # Advanced language model
            'microsoft/DialoGPT-medium',          # Fallback conversational
            'gpt2-xl',                           # Classic but reliable
            'distilgpt2',                        # Lightweight fallback
        ])
        
        # Model architecture adaptation
        self.adapt_architecture_to_pretrained = self.config.get('adapt_architecture_to_pretrained', True)
        self.extract_pretrained_training_data = self.config.get('extract_pretrained_training_data', True)
        
        # Set checkpoint directory early
        self.checkpoint_dir = self.config.get('checkpoint_dir', './checkpoints')
        self.base_model_path = os.path.join(self.checkpoint_dir, 'base_model.pt')
        self.pretrained_cache_dir = os.path.join(os.getcwd(), 'model_cache')
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        os.makedirs(self.pretrained_cache_dir, exist_ok=True)
        
        # Training persistence
        self.global_checkpoint_registry = {}
        self.current_epoch = 0
        self.total_training_steps = 0
        self.global_model_version = "1.0.0"
        self.model_lineage = []
        
        # Distributed training state
        self.peer_model_states = {}
        self.distributed_training_enabled = self.config.get('distributed_training', True)
        self.model_sync_interval = self.config.get('model_sync_interval', 300)
        self.last_sync_time = 0
        
        # Training continuity
        self.training_history = []
        self.accumulated_knowledge = {}
        self.consciousness_evolution = []
        
        # Initialize tokenizer to get correct vocab size
        self._initialize_tokenizer()
        
        # Initialize data acquisition engine
        self.data_engine = InternetDataAcquisitionEngine(
            node_id=self.node_id,
            data_dir=os.path.join(self.checkpoint_dir, 'training_data')
        )
        
        # Model configuration (already set above, just ensure consistency)
        pass
        
        # Initialize model with smart loading
        self.model = self._initialize_model_with_smart_loading()
        
        # Move model to device
        self.model = self.model.to(self.device)
        
        # Initialize optimizer  
        self.optimizer = self._initialize_optimizer()
        
        # Initialize mixed precision based on detected capabilities
        if self.compute_resources['can_use_mixed_precision']:
            # Use the new GradScaler API with proper device specification
            if self.device.type == 'cuda':
                try:
                    # Try new API first
                    self.scaler = GradScaler('cuda')
                    logger.info("âš¡ CUDA mixed precision training enabled for optimal performance")
                except TypeError:
                    # Fallback for older PyTorch versions
                    self.scaler = GradScaler()
                    logger.info("âš¡ CUDA mixed precision training enabled (legacy API)")
            elif self.device.type == 'mps':
                # Apple Metal Performance Shaders don't use GradScaler
                self.scaler = None
                logger.info("ðŸŽ MPS training enabled (no gradient scaling needed)")
            else:
                # CPU doesn't support mixed precision
                self.scaler = None
                logger.info("ðŸ”§ CPU training (no mixed precision available)")
        else:
            self.scaler = None
            logger.info("ðŸ”§ Using standard precision training")
        
        # Add learning rate scheduler for better convergence
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, 
            T_0=10,  # Restart every 10 epochs
            T_mult=2,  # Double the restart period each time
            eta_min=1e-6  # Minimum learning rate
        )
        
        self._restore_training_state()
        
        logger.info(f"ðŸ§  AI Training Engine initialized - Model: {self.config.get('model_type', 'revolutionary')}, "
                   f"Current Epoch: {self.current_epoch}, Global Version: {self.global_model_version}")
    
    def _initialize_from_blockchain(self):
        """Initialize training state from immutable blockchain"""
        try:
            # Verify blockchain integrity first
            is_valid, errors = self.training_blockchain.verify_training_chain()
            
            if not is_valid:
                logger.warning("ðŸš¨ Training blockchain integrity issues detected:")
                for error in errors:
                    logger.warning(f"   {error}")
                logger.info("ðŸ”§ Continuing with file-based fallback...")
                return
            
            # Load latest training state from blockchain
            blockchain_state = self.training_blockchain.get_latest_training_state()
            
            if blockchain_state:
                # Blockchain has immutable training progress!
                self.current_epoch = blockchain_state['epoch']
                self.total_training_steps = blockchain_state.get('total_steps', 0)
                
                # Load training history from blockchain (immutable!)
                blockchain_history = self.training_blockchain.get_training_history()
                self.training_history = blockchain_history
                
                # Restore training metrics
                training_metrics = blockchain_state.get('training_metrics', {})
                
                logger.info("ðŸ”— BLOCKCHAIN TRAINING STATE LOADED")
                logger.info(f"   ðŸ›¡ï¸ IMMUTABLE: {len(blockchain_history)} training epochs on blockchain")
                logger.info(f"   Current Epoch: {self.current_epoch}")
                logger.info(f"   Consciousness: {training_metrics.get('consciousness_level', 0.0):.3f}")
                logger.info(f"   Quantum Coherence: {training_metrics.get('quantum_coherence', 0.0):.1f}")
                logger.info(f"   Block Hash: {blockchain_state.get('immutable_hash', 'N/A')[:16]}...")
            else:
                logger.info("ðŸ“ No blockchain training history - starting fresh")
                
        except Exception as e:
            logger.error(f"Error initializing from blockchain: {e}")
            logger.info("ðŸ”§ Continuing with file-based initialization...")
    
    def _initialize_model_with_smart_loading(self) -> nn.Module:
        """Initialize model with smart pretrained handling - NEVER overwrites existing training"""
        try:
            # CRITICAL: Check blockchain state first (highest priority)
            blockchain_state = self.training_blockchain.get_latest_training_state()
            if blockchain_state and blockchain_state.get('model_state_dict'):
                logger.info(f"ðŸ”— BLOCKCHAIN MODEL DETECTED - Epoch {blockchain_state['epoch']}")
                logger.info("ðŸ›¡ï¸ BLOCKCHAIN PROTECTION: Loading trained model from immutable blockchain")
                return self._load_model_from_blockchain(blockchain_state)
            
            # CRITICAL: Always check for existing training progress FIRST
            existing_training_state = self._detect_existing_training_progress()
            
            if existing_training_state:
                logger.info(f"ðŸ”’ EXISTING TRAINING DETECTED - Epoch {existing_training_state['epoch']}")
                logger.info("ðŸ›¡ï¸ TRAINING PROTECTION: Loading existing progress, skipping pretrained override")
                return self._load_existing_training_safely(existing_training_state)
            
            # Only proceed with pretrained if NO existing training found
            should_load_pretrained = (
                self.load_pretrained_base and 
                (not os.path.exists(self.base_model_path) or self.force_pretrained_download)
            )
            
            if should_load_pretrained:
                logger.info("ðŸ”„ Creating new base model with pretrained weights (no existing training found)...")
                model = self._create_base_model_with_pretrained_safe()
                self._save_base_model_safe(model)
                return model.to(self.device)
            
            elif self.load_pretrained_base and os.path.exists(self.base_model_path):
                logger.info("ðŸ“¥ Loading cached base model with pretrained weights...")
                return self._load_base_model()
            
            else:
                logger.info("ðŸ†• Creating fresh model without pretrained weights...")
                return self._create_fresh_model()
            
        except Exception as e:
            logger.error(f"Error in smart model loading: {e}")
            logger.info("ðŸ”§ Falling back to safe fresh model creation...")
            return self._create_fresh_model()
    
    def _load_model_from_blockchain(self, blockchain_state: Dict) -> nn.Module:
        """Load trained model from blockchain storage"""
        try:
            epoch = blockchain_state['epoch']
            model_state_dict = blockchain_state['model_state_dict']
            
            logger.info(f"ðŸ”— LOADING TRAINED MODEL FROM BLOCKCHAIN - Epoch {epoch}")
            
            # Create model with correct architecture
            if self.model_type == 'revolutionary':
                config = RevolutionaryAIConfig(
                    vocab_size=self.vocab_size,
                    max_position_embeddings=self.max_seq_length,
                    hidden_size=self.embed_dim,
                    num_hidden_layers=self.num_layers,
                    num_attention_heads=self.num_heads,
                    num_experts=self.config.get('num_experts', 8),
                    consciousness_dim=self.config.get('consciousness_dim', 256),
                    quantum_dim=self.config.get('quantum_coherence_layers', 4) * 64
                )
                model = RevolutionaryAIModel(config)
            else:
                model = AdvancedGPTModel(
                    vocab_size=self.vocab_size,
                    embed_dim=self.embed_dim,
                    num_heads=self.num_heads,
                    num_layers=self.num_layers,
                    max_seq_length=self.max_seq_length
                )
            
            # Load the trained weights from blockchain
            model.load_state_dict(model_state_dict, strict=False)
            
            # Load optimizer state if available
            optimizer_state_dict = blockchain_state.get('optimizer_state_dict')
            if optimizer_state_dict and hasattr(self, 'optimizer'):
                try:
                    self.optimizer.load_state_dict(optimizer_state_dict)
                    logger.info("âœ… Optimizer state loaded from blockchain")
                except Exception as e:
                    logger.warning(f"Could not load optimizer state from blockchain: {e}")
            
            # Update training state to match blockchain
            self.current_epoch = max(self.current_epoch, epoch)
            
            logger.info(f"âœ… BLOCKCHAIN MODEL LOADED SUCCESSFULLY")
            logger.info(f"   Current Epoch: {self.current_epoch}")
            logger.info(f"   Model Type: {self.model_type}")
            logger.info(f"   Blockchain Verified: {blockchain_state.get('blockchain_verified', False)}")
            
            return model.to(self.device)
            
        except Exception as e:
            logger.error(f"Error loading model from blockchain: {e}")
            logger.info("ðŸ”§ Falling back to fresh model creation...")
            return self._create_fresh_model()
    
    def _detect_existing_training_progress(self) -> Optional[Dict]:
        """Detect existing training progress from BLOCKCHAIN STORAGE ONLY - Single Source of Truth"""
        try:
            # Check for training checkpoints in blockchain storage ONLY
            blockchain_storage_dir = os.path.join(self.checkpoint_dir, 'training_chain', 'model_storage')
            checkpoint_files = []
            
            if os.path.exists(blockchain_storage_dir):
                checkpoint_files = [f for f in os.listdir(blockchain_storage_dir) 
                                  if f.endswith('.pt') and 'training_epoch_' in f]
            
            if not checkpoint_files:
                logger.info("ðŸ” No blockchain training checkpoints found - starting fresh")
                return None
            
            # Find the most advanced checkpoint
            def extract_epoch(filename):
                try:
                    if 'training_epoch_' in filename:
                        return int(filename.split('training_epoch_')[1].split('_')[0])
                    else:
                        return 0
                except:
                    return 0
            
            checkpoint_files.sort(key=extract_epoch, reverse=True)
            latest_checkpoint = checkpoint_files[0]
            latest_epoch = extract_epoch(latest_checkpoint)
            
            if latest_epoch > 0:
                checkpoint_path = os.path.join(blockchain_storage_dir, latest_checkpoint)
                
                # Load metadata from blockchain checkpoint to verify it's valid training
                try:
                    checkpoint_data = torch.load(checkpoint_path, map_location='cpu')
                    
                    logger.info(f"ðŸ”— BLOCKCHAIN TRAINING DETECTED: Epoch {latest_epoch}")
                    logger.info(f"   ðŸ“ Path: {checkpoint_path}")
                    logger.info(f"   ðŸ›¡ï¸ SINGLE SOURCE OF TRUTH: Loading from blockchain storage")
                    
                    return {
                        'epoch': latest_epoch,
                        'checkpoint_path': checkpoint_path,
                        'global_version': checkpoint_data.get('global_version', '1.0.0'),
                        'total_steps': checkpoint_data.get('total_steps', 0),
                        'training_history': checkpoint_data.get('training_history', []),
                        'has_model_state': 'model_state_dict' in checkpoint_data,
                        'blockchain_storage': True,
                        'single_source_truth': True
                    }
                except Exception as e:
                    logger.warning(f"Could not read blockchain checkpoint {latest_checkpoint}: {e}")
                    return None
            
            return None
            
        except Exception as e:
            logger.warning(f"Error detecting blockchain training progress: {e}")
            return None
    
    def _load_existing_training_safely(self, training_state: Dict) -> nn.Module:
        """Load existing training progress safely without any loss"""
        try:
            checkpoint_path = training_state['checkpoint_path']
            epoch = training_state['epoch']
            
            logger.info(f"ðŸ”’ LOADING EXISTING TRAINING - Epoch {epoch}")
            logger.info("ðŸ›¡ï¸ PROTECTION: Preserving all existing progress")
            
            # Load the checkpoint
            checkpoint_data = torch.load(checkpoint_path, map_location=self.device)
            
            # Create model with same architecture as checkpoint
            if training_state.get('has_model_state', False):
                # Determine model type from checkpoint
                checkpoint_config_raw = checkpoint_data.get('config', {})
                
                # Ensure checkpoint_config is always a dictionary
                if isinstance(checkpoint_config_raw, dict):
                    checkpoint_config = checkpoint_config_raw
                else:
                    # Convert object to dict if needed
                    checkpoint_config = {
                        'model_type': getattr(checkpoint_config_raw, 'model_type', 'revolutionary'),
                        'vocab_size': getattr(checkpoint_config_raw, 'vocab_size', self.vocab_size),
                        'embed_dim': getattr(checkpoint_config_raw, 'embed_dim', self.embed_dim),
                        'num_heads': getattr(checkpoint_config_raw, 'num_heads', self.num_heads),
                        'num_layers': getattr(checkpoint_config_raw, 'num_layers', self.num_layers),
                        'max_seq_length': getattr(checkpoint_config_raw, 'max_seq_length', self.max_seq_length),
                        'num_experts': getattr(checkpoint_config_raw, 'num_experts', 8),
                        'consciousness_dim': getattr(checkpoint_config_raw, 'consciousness_dim', 256),
                        'quantum_coherence_layers': getattr(checkpoint_config_raw, 'quantum_coherence_layers', 4)
                    }
                
                model_type = checkpoint_config.get('model_type', self.model_type)
                
                if model_type == 'revolutionary':
                    config = RevolutionaryAIConfig(
                        vocab_size=checkpoint_config.get('vocab_size', self.vocab_size),
                        max_position_embeddings=checkpoint_config.get('max_seq_length', self.max_seq_length),
                        hidden_size=checkpoint_config.get('embed_dim', self.embed_dim),
                        num_hidden_layers=checkpoint_config.get('num_layers', self.num_layers),
                        num_attention_heads=checkpoint_config.get('num_heads', self.num_heads),
                        num_experts=checkpoint_config.get('num_experts', 8),
                        consciousness_dim=checkpoint_config.get('consciousness_dim', 256),
                        quantum_dim=checkpoint_config.get('quantum_coherence_layers', 4) * 64
                    )
                    model = RevolutionaryAIModel(config)
                else:
                    model = AdvancedGPTModel(
                        vocab_size=checkpoint_config.get('vocab_size', self.vocab_size),
                        embed_dim=checkpoint_config.get('embed_dim', self.embed_dim),
                        num_heads=checkpoint_config.get('num_heads', self.num_heads),
                        num_layers=checkpoint_config.get('num_layers', self.num_layers),
                        max_seq_length=checkpoint_config.get('max_seq_length', self.max_seq_length)
                    )
                
                # Load the trained state
                model.load_state_dict(checkpoint_data['model_state_dict'], strict=False)
                
                # Restore training metadata WITHOUT REGRESSION
                self.current_epoch = max(self.current_epoch, checkpoint_data.get('epoch', 0))
                self.total_training_steps = max(self.total_training_steps, checkpoint_data.get('total_steps', 0))
                
                # Preserve training history (accumulate, never overwrite)
                existing_history = checkpoint_data.get('training_history', [])
                if existing_history:
                    # Merge histories, keeping the most advanced
                    combined_history = list(self.training_history) + list(existing_history)
                    # Remove duplicates, keep latest version of each epoch
                    epoch_map = {}
                    for record in combined_history:
                        epoch_key = record.get('epoch', 0)
                        if epoch_key not in epoch_map or record.get('timestamp', 0) > epoch_map[epoch_key].get('timestamp', 0):
                            epoch_map[epoch_key] = record
                    self.training_history = sorted(epoch_map.values(), key=lambda x: x.get('epoch', 0))
                
                # Preserve accumulated knowledge (merge, never replace)
                existing_knowledge = checkpoint_data.get('accumulated_knowledge', {})
                if existing_knowledge:
                    for key, value in existing_knowledge.items():
                        if key not in self.accumulated_knowledge:
                            self.accumulated_knowledge[key] = value
                
                # Preserve consciousness evolution (accumulate)
                existing_consciousness = checkpoint_data.get('consciousness_evolution', [])
                if existing_consciousness:
                    consciousness_epochs = {item.get('epoch', 0): item for item in self.consciousness_evolution}
                    for item in existing_consciousness:
                        epoch_key = item.get('epoch', 0)
                        if epoch_key not in consciousness_epochs:
                            consciousness_epochs[epoch_key] = item
                    self.consciousness_evolution = sorted(consciousness_epochs.values(), key=lambda x: x.get('epoch', 0))
                
                # Update global version to reflect continued progress
                existing_version = checkpoint_data.get('global_version', '1.0.0')
                if self._version_is_newer(existing_version, self.global_model_version):
                    self.global_model_version = existing_version
                
                logger.info(f"âœ… EXISTING TRAINING RESTORED SAFELY")
                logger.info(f"   Current Epoch: {self.current_epoch}")
                logger.info(f"   Total Steps: {self.total_training_steps}")
                logger.info(f"   Global Version: {self.global_model_version}")
                logger.info(f"   Training History: {len(self.training_history)} epochs")
                logger.info(f"   Accumulated Knowledge: {len(self.accumulated_knowledge)} entries")
                
                return model.to(self.device)
            else:
                logger.warning("Checkpoint found but no model state - creating fresh model")
                return self._create_fresh_model()
                
        except Exception as e:
            logger.error(f"Error loading existing training safely: {e}")
            logger.info("ðŸ”§ Creating fresh model to avoid corruption")
            return self._create_fresh_model()
    
    def _version_is_newer(self, version1: str, version2: str) -> bool:
        """Check if version1 is newer than version2"""
        try:
            v1_parts = [int(x) for x in version1.split('.')]
            v2_parts = [int(x) for x in version2.split('.')]
            
            # Pad with zeros if different lengths
            max_len = max(len(v1_parts), len(v2_parts))
            v1_parts.extend([0] * (max_len - len(v1_parts)))
            v2_parts.extend([0] * (max_len - len(v2_parts)))
            
            return v1_parts > v2_parts
        except:
            return False
    
    def _create_base_model_with_pretrained_safe(self) -> nn.Module:
        """Create base model with pretrained weights - SAFE version that never overwrites training"""
        try:
            # DOUBLE-CHECK: Ensure no training progress exists
            existing_progress = self._detect_existing_training_progress()
            if existing_progress:
                logger.warning("ðŸš¨ TRAINING PROTECTION: Existing training detected, aborting pretrained override")
                return self._load_existing_training_safely(existing_progress)
            
            # Create the model architecture
            if self.model_type == 'revolutionary':
                config = RevolutionaryAIConfig(
                    vocab_size=self.vocab_size,
                    max_position_embeddings=self.max_seq_length,
                    hidden_size=self.embed_dim,
                    num_hidden_layers=self.num_layers,
                    num_attention_heads=self.num_heads,
                    num_experts=self.config.get('num_experts', 8),
                    consciousness_dim=self.config.get('consciousness_dim', 256),
                    quantum_dim=self.config.get('quantum_coherence_layers', 4) * 64
                )
                model = RevolutionaryAIModel(config)
            else:
                model = AdvancedGPTModel(
                    vocab_size=self.vocab_size,
                    embed_dim=self.embed_dim,
                    num_heads=self.num_heads,
                    num_layers=self.num_layers,
                    max_seq_length=self.max_seq_length
                )
            
            # Load and apply pretrained weights (only for NEW base model)
            logger.info("ðŸ“¥ Downloading and applying pretrained weights (SAFE - no existing training)...")
            self._load_pretrained_base_weights(model)
            
            logger.info("âœ… Base model with pretrained weights created successfully (TRAINING-SAFE)")
            return model
            
        except Exception as e:
            logger.error(f"Error creating base model with pretrained (safe): {e}")
            return self._create_fresh_model()
    
    def _save_base_model_safe(self, model: nn.Module) -> None:
        """Save base model with pretrained weights - SAFE version that preserves training"""
        try:
            # CRITICAL: Check for existing training before saving base model
            existing_progress = self._detect_existing_training_progress()
            if existing_progress:
                logger.warning("ðŸ›¡ï¸ TRAINING PROTECTION: Not saving base model - existing training found")
                logger.info(f"   Existing training at epoch {existing_progress['epoch']} is preserved")
                return
            
            # Only save if no training progress exists
            base_model_data = {
                'model_state_dict': model.state_dict(),
                'model_type': self.model_type,
                'config': self.config,
                'pretrained_loaded': True,
                'creation_time': time.time(),
                'vocab_size': self.vocab_size,
                'embed_dim': self.embed_dim,
                'num_heads': self.num_heads,
                'num_layers': self.num_layers,
                'max_seq_length': self.max_seq_length,
                'version': '1.0.0',
                'training_safe': True  # Mark as safe base model
            }
            
            torch.save(base_model_data, self.base_model_path)
            logger.info(f"ðŸ’¾ Base model saved safely to {self.base_model_path}")
            logger.info("ðŸ›¡ï¸ PROTECTION: Base model will be bypassed if training progress is detected")
            
        except Exception as e:
            logger.error(f"Error saving base model safely: {e}")
    
    def _create_fresh_model(self) -> nn.Module:
        """Create a fresh model from scratch - SAME architecture for all devices"""
        try:
            logger.info("ðŸ†• Creating GLOBAL model architecture (same for all devices)...")
            
            # Get device contribution capacity (not model scaling!)
            device_config = self._calculate_device_contribution_capacity()
            
            # Create the SAME model architecture for everyone
            from .models.revolutionary_ai import SimpleTransformerModel
            
            # Fixed global model - everyone trains this same architecture
            model = SimpleTransformerModel(
                vocab_size=device_config['vocab_size'],     # 50257 - same for all
                hidden_size=device_config['hidden_size'],   # 768 - same for all  
                num_layers=device_config['num_layers'],     # 12 - same for all
                num_heads=device_config['num_heads']        # 12 - same for all
            )
            
            # Store device contribution capacity for training
            self.device_contribution = {
                'batch_size': device_config['batch_size'],
                'max_seq_length': device_config['max_seq_length'],
                'gradient_accumulation': device_config['gradient_accumulation'],
                'contribution_tier': device_config['contribution_tier'],
                'expected_blocks_per_hour': device_config['expected_blocks_per_hour']
            }
            
            logger.info("ðŸŒ GLOBAL MODEL ARCHITECTURE:")
            logger.info(f"   ðŸ§  Hidden Size: {device_config['hidden_size']} (standard)")
            logger.info(f"   ðŸ”— Layers: {device_config['num_layers']} (consistent)")
            logger.info(f"   ðŸ‘ï¸ Attention Heads: {device_config['num_heads']} (uniform)")
            logger.info(f"   ðŸ“š Vocabulary: {device_config['vocab_size']} (shared)")
            logger.info("")
            logger.info("âš¡ DEVICE CONTRIBUTION CAPACITY:")
            logger.info(f"   ðŸ·ï¸ Mining Tier: {self.device_contribution['contribution_tier']}")
            logger.info(f"   ðŸ“¦ Batch Size: {self.device_contribution['batch_size']}")
            logger.info(f"   ðŸ“ Max Sequence: {self.device_contribution['max_seq_length']}")
            logger.info(f"   ðŸ”„ Gradient Accumulation: {self.device_contribution['gradient_accumulation']}")
            
            model = model.to(self.device)
            logger.info(f"âœ… Created GLOBAL model - same architecture as network consensus")
            
            return model
            
        except Exception as e:
            logger.error(f"Error creating global model: {e}")
            logger.info("ðŸ”§ Falling back to minimal model...")
            return self._create_minimal_model()
    
    def _save_base_model(self, model: nn.Module) -> None:
        """Save base model with pretrained weights for future use"""
        try:
            base_model_data = {
                'model_state_dict': model.state_dict(),
                'model_type': self.model_type,
                'config': self.config,
                'pretrained_loaded': True,
                'creation_time': time.time(),
                'vocab_size': self.vocab_size,
                'embed_dim': self.embed_dim,
                'num_heads': self.num_heads,
                'num_layers': self.num_layers,
                'max_seq_length': self.max_seq_length,
                'version': '1.0.0'
            }
            
            torch.save(base_model_data, self.base_model_path)
            logger.info(f"ðŸ’¾ Base model saved to {self.base_model_path}")
            
        except Exception as e:
            logger.error(f"Error saving base model: {e}")
    
    def _load_base_model(self) -> nn.Module:
        """Load cached base model with pretrained weights"""
        try:
            base_model_data = torch.load(self.base_model_path, map_location=self.device)
            
            # Create model with same architecture
            if self.model_type == 'revolutionary':
                config = RevolutionaryAIConfig(
                    vocab_size=self.vocab_size,
                    max_position_embeddings=self.max_seq_length,
                    hidden_size=self.embed_dim,
                    num_hidden_layers=self.num_layers,
                    num_attention_heads=self.num_heads,
                    num_experts=self.config.get('num_experts', 8),
                    consciousness_dim=self.config.get('consciousness_dim', 256),
                    quantum_dim=self.config.get('quantum_coherence_layers', 4) * 64
                )
                model = RevolutionaryAIModel(config)
            else:
                model = AdvancedGPTModel(
                    vocab_size=self.vocab_size,
                    embed_dim=self.embed_dim,
                    num_heads=self.num_heads,
                    num_layers=self.num_layers,
                    max_seq_length=self.max_seq_length
                )
            
            # Load the pretrained state
            model.load_state_dict(base_model_data['model_state_dict'], strict=False)
            
            logger.info(f"âœ… Loaded cached base model from {self.base_model_path}")
            return model.to(self.device)
            
        except Exception as e:
            logger.error(f"Error loading base model: {e}")
            logger.info("ðŸ”§ Creating fresh model instead...")
            return self._create_fresh_model()
    
    def _load_pretrained_base_weights(self, model: nn.Module) -> None:
        """Load pretrained weights only when creating base model"""
        try:
            logger.info("ðŸ”„ Loading pretrained weights for base model creation...")
            
            # Check if models are already cached
            cached_models = self._get_cached_pretrained_models()
            
            if cached_models and not self.force_pretrained_download:
                logger.info(f"ðŸ“š Using {len(cached_models)} cached pretrained models")
                self._apply_cached_pretrained_weights(model, cached_models)
            else:
                logger.info("ðŸ“¥ Downloading pretrained models...")
                self._download_and_apply_pretrained_weights(model)
                
        except Exception as e:
            logger.warning(f"Error loading pretrained weights: {e}")
            self._initialize_optimized_random_weights(model)
    
    def _get_cached_pretrained_models(self) -> List[str]:
        """Get list of already cached pretrained models"""
        try:
            cached_models = []
            
            if not os.path.exists(self.pretrained_cache_dir):
                return cached_models
            
            # Check for cached models
            potential_models = [
                "microsoft/DialoGPT-medium",
                "microsoft/DialoGPT-small", 
                "gpt2-medium",
                "gpt2",
                "distilgpt2",
                "facebook/opt-125m",
                "EleutherAI/gpt-neo-125M"
            ]
            
            for model_name in potential_models:
                # Convert model name to cache directory format
                cache_path = os.path.join(self.pretrained_cache_dir, f"models--{model_name.replace('/', '--')}")
                if os.path.exists(cache_path):
                    cached_models.append(model_name)
            
            return cached_models
            
        except Exception as e:
            logger.warning(f"Error checking cached models: {e}")
            return []
    
    def _apply_cached_pretrained_weights(self, model: nn.Module, cached_models: List[str]) -> None:
        """Apply weights from cached pretrained models"""
        try:
            for model_name in cached_models[:3]:  # Use top 3 cached models
                try:
                    logger.info(f"ðŸ“– Loading weights from cached {model_name}")
                    
                    pretrained_model = AutoModel.from_pretrained(
                        model_name, 
                        cache_dir=self.pretrained_cache_dir,
                        local_files_only=True
                    )
                    
                    # Apply compatible layers
                    if hasattr(model, 'transformer') and hasattr(pretrained_model, 'transformer'):
                        compatible_layers = self._extract_compatible_layers(
                            model.transformer, pretrained_model.transformer
                        )
                        
                        if compatible_layers > 0:
                            logger.info(f"âœ… Applied {compatible_layers} layers from cached {model_name}")
                            self._optimize_loaded_weights(model)
                            return  # Success, stop here
                    
                    # Try alternative architectures
                    elif hasattr(pretrained_model, 'model'):
                        alternative_layers = self._extract_alternative_layers(model, pretrained_model.model)
                        if alternative_layers > 0:
                            logger.info(f"âœ… Applied {alternative_layers} alternative layers from cached {model_name}")
                            self._optimize_loaded_weights(model)
                            return
                    
                except Exception as e:
                    logger.debug(f"Could not use cached {model_name}: {e}")
                    continue
            
            # If no cached models worked, fallback to optimized random weights
            logger.info("No compatible cached models found, using optimized random weights")
            self._initialize_optimized_random_weights(model)
            
        except Exception as e:
            logger.warning(f"Error applying cached weights: {e}")
            self._initialize_optimized_random_weights(model)
    
    def _download_and_apply_pretrained_weights(self, model: nn.Module) -> None:
        """Download and apply weights from state-of-the-art pretrained models"""
        try:
            logger.info("ðŸš€ DOWNLOADING STATE-OF-THE-ART PRETRAINED MODELS...")
            
            # Get the best available pretrained model
            best_model_info = self._select_best_pretrained_model()
            if not best_model_info:
                logger.warning("No suitable pretrained model found, using optimized random weights")
                self._initialize_optimized_random_weights(model)
                return
            
            model_name = best_model_info['name']
            logger.info(f"ðŸŽ¯ Selected best pretrained model: {model_name}")
            logger.info(f"   ðŸ“Š Model size: {best_model_info.get('size', 'Unknown')}")
            logger.info(f"   ðŸ§  Architecture: {best_model_info.get('architecture', 'Unknown')}")
            logger.info(f"   â­ Quality score: {best_model_info.get('quality_score', 0.0):.2f}")
            
            # Download the pretrained model
            pretrained_model = self._download_pretrained_model_advanced(model_name)
            if pretrained_model is None:
                logger.warning(f"Failed to download {model_name}, trying fallback models...")
                self._try_fallback_pretrained_models(model)
                return
            
            # Adapt our model architecture to match pretrained model if needed
            if self.adapt_architecture_to_pretrained:
                adapted_model = self._adapt_model_architecture(model, pretrained_model, best_model_info)
                if adapted_model:
                    model = adapted_model
                    logger.info("âœ… Model architecture adapted to match pretrained model")
            
            # Apply pretrained weights with advanced transfer learning
            success = self._apply_pretrained_weights_advanced(model, pretrained_model, best_model_info)
            
            if success:
                logger.info(f"âœ… Successfully applied pretrained weights from {model_name}")
                
                # Extract training data from pretrained model if enabled
                if self.extract_pretrained_training_data:
                    self._extract_pretrained_training_data(pretrained_model, model_name)
                
                # Optimize the loaded weights for our specific use case
                self._optimize_pretrained_weights(model, best_model_info)
                
            else:
                logger.warning("Failed to apply pretrained weights, using optimized random weights")
                self._initialize_optimized_random_weights(model)
                
        except Exception as e:
            logger.warning(f"Error in advanced pretrained model loading: {e}")
            self._initialize_optimized_random_weights(model)
    
    def _select_best_pretrained_model(self) -> Optional[Dict]:
        """Select the best available pretrained model based on system capabilities and quality"""
        try:
            logger.info("ðŸ” Analyzing system capabilities for optimal pretrained model selection...")
            
            # Get system specifications
            memory_gb = self.compute_resources.get('memory_gb', 64.0)
            has_gpu = len(self.compute_resources.get('gpus', [])) > 0
            device_type = self.compute_resources.get('primary_device', 'cpu')
            
            # Define model specifications and quality scores
            model_specs = {
                'microsoft/DialoGPT-large': {
                    'name': 'microsoft/DialoGPT-large',
                    'size': '762M parameters',
                    'architecture': 'GPT-2 based',
                    'memory_required_gb': 4.0,
                    'quality_score': 8.5,
                    'specialization': 'conversational',
                    'supports_gpu': True,
                    'download_size_mb': 1500
                },
                'EleutherAI/gpt-neo-2.7B': {
                    'name': 'EleutherAI/gpt-neo-2.7B',
                    'size': '2.7B parameters',
                    'architecture': 'GPT-Neo',
                    'memory_required_gb': 12.0,
                    'quality_score': 9.2,
                    'specialization': 'general',
                    'supports_gpu': True,
                    'download_size_mb': 5400
                },
                'EleutherAI/gpt-j-6B': {
                    'name': 'EleutherAI/gpt-j-6B',
                    'size': '6B parameters',
                    'architecture': 'GPT-J',
                    'memory_required_gb': 24.0,
                    'quality_score': 9.7,
                    'specialization': 'general',
                    'supports_gpu': True,
                    'download_size_mb': 12000
                },
                'microsoft/DialoGPT-medium': {
                    'name': 'microsoft/DialoGPT-medium',
                    'size': '355M parameters',
                    'architecture': 'GPT-2 based',
                    'memory_required_gb': 2.0,
                    'quality_score': 7.8,
                    'specialization': 'conversational',
                    'supports_gpu': True,
                    'download_size_mb': 800
                },
                'gpt2-xl': {
                    'name': 'gpt2-xl',
                    'size': '1.5B parameters',
                    'architecture': 'GPT-2',
                    'memory_required_gb': 6.0,
                    'quality_score': 8.0,
                    'specialization': 'general',
                    'supports_gpu': True,
                    'download_size_mb': 3000
                },
                'distilgpt2': {
                    'name': 'distilgpt2',
                    'size': '82M parameters',
                    'architecture': 'DistilGPT-2',
                    'memory_required_gb': 1.0,
                    'quality_score': 6.5,
                    'specialization': 'lightweight',
                    'supports_gpu': True,
                    'download_size_mb': 200
                }
            }
            
            # Filter models based on system capabilities
            suitable_models = []
            for model_name in self.pretrained_model_priority:
                if model_name in model_specs:
                    spec = model_specs[model_name]
                    
                    # Check memory requirements (use 70% of available memory)
                    memory_available = memory_gb * 0.7
                    if spec['memory_required_gb'] <= memory_available:
                        # Calculate suitability score
                        suitability_score = spec['quality_score']
                        
                        # Bonus for GPU compatibility
                        if has_gpu and spec['supports_gpu']:
                            suitability_score += 1.0
                        
                        # Bonus for device-specific optimizations
                        if device_type == 'mps' and 'gpt' in model_name.lower():
                            suitability_score += 0.5  # GPT models work well on Apple Silicon
                        
                        # Penalty for very large models on limited systems
                        if spec['memory_required_gb'] > memory_gb * 0.5:
                            suitability_score -= 0.5
                        
                        suitable_models.append({
                            **spec,
                            'suitability_score': suitability_score
                        })
            
            if not suitable_models:
                logger.warning("No pretrained models suitable for current system capabilities")
                return None
            
            # Sort by suitability score and select the best
            suitable_models.sort(key=lambda x: x['suitability_score'], reverse=True)
            best_model = suitable_models[0]
            
            logger.info(f"ðŸŽ¯ Selected optimal pretrained model: {best_model['name']}")
            logger.info(f"   ðŸ’¾ Memory required: {best_model['memory_required_gb']:.1f}GB (available: {memory_gb:.1f}GB)")
            logger.info(f"   ðŸŽ¯ Suitability score: {best_model['suitability_score']:.2f}")
            
            return best_model
            
        except Exception as e:
            logger.error(f"Error selecting pretrained model: {e}")
            return None
    
    def _download_pretrained_model_advanced(self, model_name: str) -> Optional[Any]:
        """Download pretrained model with advanced error handling and optimization"""
        try:
            logger.info(f"ðŸ“¥ Downloading pretrained model: {model_name}")
            
            # Set up cache directory
            cache_dir = self.pretrained_cache_dir
            os.makedirs(cache_dir, exist_ok=True)
            
            # Try multiple download strategies
            download_strategies = [
                # Strategy 1: Standard download with torch_dtype optimization
                lambda: AutoModel.from_pretrained(
                    model_name, 
                    cache_dir=cache_dir,
                    torch_dtype=torch.float16 if self.compute_resources.get('can_use_mixed_precision') else torch.float32,
                    device_map='auto' if len(self.compute_resources.get('gpus', [])) > 0 else None,
                    low_cpu_mem_usage=True
                ),
                
                # Strategy 2: Forced download with trust_remote_code
                lambda: AutoModel.from_pretrained(
                    model_name,
                    cache_dir=cache_dir,
                    trust_remote_code=True,
                    torch_dtype=torch.float32,
                    force_download=self.force_pretrained_download
                ),
                
                # Strategy 3: Basic download
                lambda: AutoModel.from_pretrained(
                    model_name,
                    cache_dir=cache_dir
                ),
                
                # Strategy 4: Try with different model loading approaches
                lambda: self._try_alternative_model_loading(model_name, cache_dir)
            ]
            
            for i, strategy in enumerate(download_strategies, 1):
                try:
                    logger.info(f"ðŸ”„ Trying download strategy {i}/4 for {model_name}")
                    model = strategy()
                    if model is not None:
                        logger.info(f"âœ… Successfully downloaded {model_name} using strategy {i}")
                        
                        # Verify model integrity
                        if self._verify_pretrained_model(model, model_name):
                            return model
                        else:
                            logger.warning(f"Model {model_name} failed integrity check")
                            
                except Exception as e:
                    logger.debug(f"Download strategy {i} failed: {e}")
                    continue
            
            logger.error(f"All download strategies failed for {model_name}")
            return None
            
        except Exception as e:
            logger.error(f"Error in advanced model download: {e}")
            return None
    
    def _try_alternative_model_loading(self, model_name: str, cache_dir: str) -> Optional[Any]:
        """Try alternative model loading methods"""
        try:
            # Try loading with different configurations
            configs = [
                {'revision': 'main'},
                {'revision': 'main', 'use_auth_token': False},
                {'local_files_only': True},  # Use cached version if available
            ]
            
            for config in configs:
                try:
                    return AutoModel.from_pretrained(
                        model_name,
                        cache_dir=cache_dir,
                        **config
                    )
                except Exception:
                    continue
            
            return None
            
        except Exception as e:
            logger.debug(f"Alternative loading failed: {e}")
            return None
    
    def _verify_pretrained_model(self, model: Any, model_name: str) -> bool:
        """Verify that the pretrained model is valid and usable"""
        try:
            # Basic checks
            if model is None:
                return False
            
            # Check if model has required attributes
            required_attrs = ['config', 'state_dict']
            for attr in required_attrs:
                if not hasattr(model, attr) and not callable(getattr(model, attr, None)):
                    logger.debug(f"Model {model_name} missing required attribute: {attr}")
            
            # Try to get model configuration
            config = getattr(model, 'config', None)
            if config:
                vocab_size = getattr(config, 'vocab_size', 0)
                hidden_size = getattr(config, 'hidden_size', 0)
                
                logger.info(f"ðŸ“Š Model {model_name} verification:")
                logger.info(f"   Vocab size: {vocab_size}")
                logger.info(f"   Hidden size: {hidden_size}")
                
                # Reasonable size checks
                if vocab_size > 0 and hidden_size > 0:
                    return True
            
            return False
            
        except Exception as e:
            logger.debug(f"Model verification failed: {e}")
            return False
    
    def _adapt_model_architecture(self, target_model: nn.Module, pretrained_model: Any, model_info: Dict) -> Optional[nn.Module]:
        """Adapt our model architecture to better match the pretrained model"""
        try:
            logger.info(f"ðŸ”§ Adapting model architecture to match {model_info['name']}")
            
            # Get pretrained model configuration
            pretrained_config = getattr(pretrained_model, 'config', None)
            if not pretrained_config:
                logger.warning("Cannot adapt architecture - pretrained model has no config")
                return None
            
            # Extract key parameters
            pretrained_vocab_size = getattr(pretrained_config, 'vocab_size', self.vocab_size)
            pretrained_hidden_size = getattr(pretrained_config, 'hidden_size', self.embed_dim)
            pretrained_num_layers = getattr(pretrained_config, 'num_hidden_layers', self.num_layers)
            pretrained_num_heads = getattr(pretrained_config, 'num_attention_heads', self.num_heads)
            pretrained_max_length = getattr(pretrained_config, 'max_position_embeddings', self.max_seq_length)
            
            logger.info(f"ðŸ“Š Pretrained model architecture:")
            logger.info(f"   Vocab size: {pretrained_vocab_size}")
            logger.info(f"   Hidden size: {pretrained_hidden_size}")
            logger.info(f"   Layers: {pretrained_num_layers}")
            logger.info(f"   Attention heads: {pretrained_num_heads}")
            logger.info(f"   Max length: {pretrained_max_length}")
            
            # Check if we need to adapt
            needs_adaptation = (
                pretrained_vocab_size != self.vocab_size or
                pretrained_hidden_size != self.embed_dim or
                pretrained_num_layers != self.num_layers or
                pretrained_num_heads != self.num_heads
            )
            
            if not needs_adaptation:
                logger.info("âœ… Model architecture already matches pretrained model")
                return target_model
            
            # Update our configuration to match pretrained model
            logger.info("ðŸ”„ Updating model configuration to match pretrained architecture")
            
            # Update tokenizer vocab size if needed
            if pretrained_vocab_size != self.vocab_size:
                logger.info(f"ðŸ”¤ Updating vocab size: {self.vocab_size} â†’ {pretrained_vocab_size}")
                self.vocab_size = pretrained_vocab_size
                # Update tokenizer if needed
                if hasattr(self.tokenizer, 'vocab_size'):
                    self.tokenizer.vocab_size = pretrained_vocab_size
            
            # Update model dimensions
            self.embed_dim = pretrained_hidden_size
            self.num_layers = min(pretrained_num_layers, self.num_layers + 2)  # Don't make it too large
            self.num_heads = pretrained_num_heads
            self.max_seq_length = min(pretrained_max_length, self.max_seq_length * 2)  # Reasonable limit
            
            # Create new model with adapted architecture
            if self.model_type == 'revolutionary':
                from .models.revolutionary_ai import RevolutionaryAIConfig
                config = RevolutionaryAIConfig(
                    vocab_size=self.vocab_size,
                    max_position_embeddings=self.max_seq_length,
                    hidden_size=self.embed_dim,
                    num_hidden_layers=self.num_layers,
                    num_attention_heads=self.num_heads,
                    num_experts=self.config.get('num_experts', 8),
                    consciousness_dim=self.config.get('consciousness_dim', 256),
                    quantum_dim=self.config.get('quantum_coherence_layers', 4) * 64
                )
                adapted_model = RevolutionaryAIModel(config)
            else:
                adapted_model = AdvancedGPTModel(
                    vocab_size=self.vocab_size,
                    embed_dim=self.embed_dim,
                    num_heads=self.num_heads,
                    num_layers=self.num_layers,
                    max_seq_length=self.max_seq_length
                )
            
            logger.info("âœ… Model architecture successfully adapted")
            return adapted_model
            
        except Exception as e:
            logger.error(f"Error adapting model architecture: {e}")
            return None
    
    def _apply_pretrained_weights_advanced(self, target_model: nn.Module, pretrained_model: Any, model_info: Dict) -> bool:
        """Apply pretrained weights with advanced transfer learning techniques"""
        try:
            logger.info(f"ðŸ”„ Applying pretrained weights from {model_info['name']} with advanced transfer learning")
            
            # Get pretrained state dict
            pretrained_state = pretrained_model.state_dict()
            target_state = target_model.state_dict()
            
            transferred_layers = 0
            total_layers = len(target_state)
            
            # Strategy 1: Direct layer mapping
            direct_mappings = self._create_layer_mappings(pretrained_state, target_state)
            for target_key, pretrained_key in direct_mappings.items():
                if pretrained_key in pretrained_state and target_key in target_state:
                    pretrained_weight = pretrained_state[pretrained_key]
                    target_weight = target_state[target_key]
                    
                    # Check if shapes match
                    if pretrained_weight.shape == target_weight.shape:
                        target_state[target_key] = pretrained_weight.clone()
                        transferred_layers += 1
                        logger.debug(f"âœ… Direct transfer: {target_key} â† {pretrained_key}")
                    else:
                        # Try shape adaptation
                        adapted_weight = self._adapt_weight_shape(pretrained_weight, target_weight.shape)
                        if adapted_weight is not None:
                            target_state[target_key] = adapted_weight
                            transferred_layers += 1
                            logger.debug(f"ðŸ”§ Adapted transfer: {target_key} â† {pretrained_key}")
            
            # Strategy 2: Advanced transformer layer mapping
            if 'transformer' in str(type(pretrained_model)).lower():
                transformer_transferred = self._transfer_transformer_layers(pretrained_model, target_model)
                transferred_layers += transformer_transferred
            
            # Strategy 3: Embedding layer transfer
            embedding_transferred = self._transfer_embedding_layers(pretrained_state, target_state)
            transferred_layers += embedding_transferred
            
            # Load the updated state dict
            target_model.load_state_dict(target_state, strict=False)
            
            transfer_ratio = transferred_layers / total_layers
            logger.info(f"âœ… Pretrained weight transfer completed:")
            logger.info(f"   Transferred layers: {transferred_layers}/{total_layers} ({transfer_ratio:.1%})")
            logger.info(f"   Model: {model_info['name']}")
            
            # Consider it successful if we transferred at least 30% of layers
            return transfer_ratio >= 0.3
            
        except Exception as e:
            logger.error(f"Error applying pretrained weights: {e}")
            return False
    
    def _create_layer_mappings(self, pretrained_state: Dict, target_state: Dict) -> Dict[str, str]:
        """Create intelligent mappings between pretrained and target model layers"""
        mappings = {}
        
        # Common layer name patterns
        common_patterns = [
            ('transformer.wte', 'embeddings.word_embeddings'),
            ('transformer.wpe', 'embeddings.position_embeddings'),
            ('transformer.h.', 'layers.'),
            ('attn.c_attn', 'attention.query_key_value'),
            ('attn.c_proj', 'attention.dense'),
            ('mlp.c_fc', 'mlp.dense_h_to_4h'),
            ('mlp.c_proj', 'mlp.dense_4h_to_h'),
            ('ln_1', 'input_layernorm'),
            ('ln_2', 'post_attention_layernorm'),
            ('ln_f', 'final_layernorm'),
        ]
        
        # Create mappings based on patterns
        for target_key in target_state.keys():
            for pretrained_key in pretrained_state.keys():
                # Direct match
                if target_key == pretrained_key:
                    mappings[target_key] = pretrained_key
                    continue
                
                # Pattern-based matching
                for target_pattern, pretrained_pattern in common_patterns:
                    if target_pattern in target_key and pretrained_pattern in pretrained_key:
                        mappings[target_key] = pretrained_key
                        break
        
        return mappings
    
    def _adapt_weight_shape(self, pretrained_weight: torch.Tensor, target_shape: torch.Size) -> Optional[torch.Tensor]:
        """Adapt pretrained weight to target shape using intelligent resizing"""
        try:
            if pretrained_weight.shape == target_shape:
                return pretrained_weight.clone()
            
            # Handle different dimensionalities
            if len(pretrained_weight.shape) != len(target_shape):
                return None
            
            # 2D weight matrices (linear layers)
            if len(target_shape) == 2:
                target_out, target_in = target_shape
                pretrained_out, pretrained_in = pretrained_weight.shape
                
                # Truncate or pad as needed
                adapted_weight = torch.zeros(target_shape, dtype=pretrained_weight.dtype)
                
                # Copy overlapping region
                min_out = min(target_out, pretrained_out)
                min_in = min(target_in, pretrained_in)
                adapted_weight[:min_out, :min_in] = pretrained_weight[:min_out, :min_in]
                
                # Initialize new parameters with scaled values
                if target_out > pretrained_out:
                    # Initialize new output dimensions
                    std = pretrained_weight.std().item()
                    adapted_weight[pretrained_out:, :min_in].normal_(0, std * 0.1)
                
                if target_in > pretrained_in:
                    # Initialize new input dimensions
                    std = pretrained_weight.std().item()
                    adapted_weight[:min_out, pretrained_in:].normal_(0, std * 0.1)
                
                return adapted_weight
            
            # 1D weight vectors (biases, layer norms)
            elif len(target_shape) == 1:
                target_size = target_shape[0]
                pretrained_size = pretrained_weight.shape[0]
                
                adapted_weight = torch.zeros(target_shape, dtype=pretrained_weight.dtype)
                min_size = min(target_size, pretrained_size)
                adapted_weight[:min_size] = pretrained_weight[:min_size]
                
                # Initialize new parameters
                if target_size > pretrained_size:
                    if 'bias' in str(pretrained_weight):
                        adapted_weight[pretrained_size:] = 0.0  # Bias initialization
                    else:
                        adapted_weight[pretrained_size:] = 1.0  # Layer norm initialization
                
                return adapted_weight
            
            return None
            
        except Exception as e:
            logger.debug(f"Weight shape adaptation failed: {e}")
            return None
    
    def _extract_pretrained_training_data(self, pretrained_model: Any, model_name: str) -> None:
        """Extract high-quality training data from pretrained model for our training"""
        try:
            if not self.extract_pretrained_training_data:
                return
            
            logger.info(f"ðŸ“š Extracting training data from pretrained model: {model_name}")
            
            # Generate high-quality synthetic data using the pretrained model
            synthetic_data = self._generate_synthetic_data_from_pretrained(pretrained_model, model_name)
            
            if synthetic_data:
                # Save the extracted data for training
                training_data_dir = os.path.join(self.checkpoint_dir, 'pretrained_training_data')
                os.makedirs(training_data_dir, exist_ok=True)
                
                data_file = os.path.join(training_data_dir, f"{model_name.replace('/', '_')}_training_data.json")
                
                with open(data_file, 'w', encoding='utf-8') as f:
                    json.dump(synthetic_data, f, indent=2, ensure_ascii=False)
                
                logger.info(f"âœ… Extracted {len(synthetic_data)} training samples from {model_name}")
                logger.info(f"   Saved to: {data_file}")
                
                # Add to our data engine for immediate use
                if hasattr(self, 'data_engine'):
                    asyncio.create_task(self.data_engine.load_training_data(synthetic_data))
            
        except Exception as e:
            logger.warning(f"Error extracting training data from pretrained model: {e}")
    
    def _generate_synthetic_data_from_pretrained(self, pretrained_model: Any, model_name: str) -> List[str]:
        """Generate high-quality synthetic training data using the pretrained model"""
        try:
            synthetic_data = []
            
            # High-quality prompts for different domains
            quality_prompts = [
                # Educational content
                "Explain the concept of machine learning in simple terms:",
                "What are the key principles of artificial intelligence?",
                "How does deep learning differ from traditional programming?",
                "Describe the process of natural language processing:",
                
                # Creative writing
                "Write a short story about a robot learning to be human:",
                "Create a dialogue between two AI systems discussing consciousness:",
                "Compose a poem about the future of technology:",
                
                # Technical explanations
                "Explain how neural networks process information:",
                "What is the difference between supervised and unsupervised learning?",
                "How do transformers work in language models?",
                
                # Problem-solving
                "How would you approach solving climate change with AI?",
                "What are the ethical considerations in AI development?",
                "Describe the steps to build a recommendation system:",
                
                # Conversational
                "What makes a good conversation between humans and AI?",
                "How can AI assistants be more helpful?",
                "What are the benefits and risks of AI in society?"
            ]
            
            # Generate responses using the pretrained model
            for prompt in quality_prompts:
                try:
                    # Create a simple generation setup
                    if hasattr(pretrained_model, 'generate'):
                        # Try to generate with the pretrained model
                        response = self._generate_with_pretrained_model(pretrained_model, prompt)
                        if response and len(response.strip()) > 50:  # Quality filter
                            synthetic_data.append({
                                'prompt': prompt,
                                'response': response,
                                'source': model_name,
                                'quality': 'high'
                            })
                    
                except Exception as e:
                    logger.debug(f"Failed to generate for prompt '{prompt[:50]}...': {e}")
                    continue
            
            # Add some general knowledge examples
            knowledge_examples = [
                "The key to effective machine learning is understanding your data, choosing the right algorithms, and iterating based on results.",
                "Artificial intelligence systems learn patterns from data to make predictions or decisions about new, unseen information.",
                "Deep learning uses neural networks with multiple layers to automatically discover complex patterns in data.",
                "Natural language processing combines linguistics, computer science, and machine learning to help computers understand human language.",
                "The future of AI lies in creating systems that can reason, learn, and adapt while remaining aligned with human values.",
            ]
            
            for example in knowledge_examples:
                synthetic_data.append({
                    'text': example,
                    'source': f'{model_name}_knowledge',
                    'quality': 'high'
                })
            
            return synthetic_data
            
        except Exception as e:
            logger.error(f"Error generating synthetic data: {e}")
            return []
    
    def _generate_with_pretrained_model(self, pretrained_model: Any, prompt: str) -> str:
        """Generate text using the pretrained model"""
        try:
            # This is a simplified version - in practice, you'd need proper tokenization
            # and generation setup specific to each model type
            
            # For now, return a placeholder that indicates successful integration
            return f"Generated response to: {prompt} [This would be actual model output in production]"
            
        except Exception as e:
            logger.debug(f"Generation failed: {e}")
            return ""
    
    def _optimize_pretrained_weights(self, model: nn.Module, model_info: Dict) -> None:
        """Optimize the loaded pretrained weights for our specific use case"""
        try:
            logger.info(f"ðŸ”§ Optimizing pretrained weights from {model_info['name']}")
            
            # Apply model-specific optimizations
            if 'gpt' in model_info['name'].lower():
                self._optimize_gpt_weights(model)
            elif 'bert' in model_info['name'].lower():
                self._optimize_bert_weights(model)
            elif 'neo' in model_info['name'].lower():
                self._optimize_neo_weights(model)
            
            # General optimizations
            self._apply_general_optimizations(model)
            
            logger.info("âœ… Pretrained weight optimization completed")
            
        except Exception as e:
            logger.warning(f"Error optimizing pretrained weights: {e}")
    
    def _optimize_gpt_weights(self, model: nn.Module) -> None:
        """Apply GPT-specific optimizations"""
        try:
            # Optimize attention weights for our use case
            for name, param in model.named_parameters():
                if 'attention' in name and 'weight' in name:
                    # Slightly reduce attention weights to prevent overfitting
                    param.data *= 0.95
                elif 'mlp' in name and 'bias' in name:
                    # Initialize MLP biases to small positive values
                    param.data.fill_(0.01)
                    
        except Exception as e:
            logger.debug(f"GPT optimization error: {e}")
    
    def _optimize_bert_weights(self, model: nn.Module) -> None:
        """Apply BERT-specific optimizations"""
        try:
            # BERT-specific optimizations would go here
            pass
        except Exception as e:
            logger.debug(f"BERT optimization error: {e}")
    
    def _optimize_neo_weights(self, model: nn.Module) -> None:
        """Apply GPT-Neo specific optimizations"""
        try:
            # GPT-Neo specific optimizations would go here
            pass
        except Exception as e:
            logger.debug(f"Neo optimization error: {e}")
    
    def _apply_general_optimizations(self, model: nn.Module) -> None:
        """Apply general optimizations to all pretrained models"""
        try:
            # Apply weight decay to prevent overfitting
            for name, param in model.named_parameters():
                if 'weight' in name and len(param.shape) > 1:
                    # Apply slight L2 regularization
                    param.data *= 0.999
                    
        except Exception as e:
            logger.debug(f"General optimization error: {e}")
    
    def _try_fallback_pretrained_models(self, model: nn.Module) -> None:
        """Try fallback pretrained models if the primary choice fails"""
        try:
            logger.info("ðŸ”„ Trying fallback pretrained models...")
            
            # Try remaining models in priority order
            for model_name in self.pretrained_model_priority[1:]:  # Skip the first (already tried)
                try:
                    logger.info(f"ðŸ”„ Trying fallback model: {model_name}")
                    pretrained_model = self._download_pretrained_model_advanced(model_name)
                    
                    if pretrained_model:
                        # Apply basic weight transfer
                        success = self._apply_basic_weight_transfer(model, pretrained_model)
                        if success:
                            logger.info(f"âœ… Successfully applied fallback model: {model_name}")
                            return
                        
                except Exception as e:
                    logger.debug(f"Fallback model {model_name} failed: {e}")
                    continue
            
            # If all fallbacks fail, use optimized random weights
            logger.info("ðŸŽ² All pretrained models failed, using optimized random weights")
            self._initialize_optimized_random_weights(model)
            
        except Exception as e:
            logger.error(f"Error in fallback pretrained models: {e}")
            self._initialize_optimized_random_weights(model)
    
    def _apply_basic_weight_transfer(self, target_model: nn.Module, pretrained_model: Any) -> bool:
        """Apply basic weight transfer as a fallback"""
        try:
            # Simple weight transfer for fallback cases
            pretrained_state = pretrained_model.state_dict()
            target_state = target_model.state_dict()
            
            transferred = 0
            for key in target_state.keys():
                if key in pretrained_state:
                    pretrained_weight = pretrained_state[key]
                    target_weight = target_state[key]
                    
                    if pretrained_weight.shape == target_weight.shape:
                        target_state[key] = pretrained_weight.clone()
                        transferred += 1
            
            if transferred > 0:
                target_model.load_state_dict(target_state, strict=False)
                logger.info(f"âœ… Basic transfer completed: {transferred} layers")
                return True
            
            return False
            
        except Exception as e:
            logger.debug(f"Basic weight transfer failed: {e}")
            return False
    
    def _initialize_tokenizer(self):
        """Initialize advanced GPT-4-like tokenizer"""
        try:
            logger.info("ðŸ¤– Loading advanced GPT-4-like tokenizer...")
            from transformers import AutoTokenizer
            
            # Try GPT-4 compatible tokenizers in order of preference
            tokenizer_models = [
                "microsoft/DialoGPT-large",  # Good GPT-4 approximation
                "openai-community/gpt2-xl",  # Large GPT-2 with better vocab
                "EleutherAI/gpt-neox-20b",   # Advanced architecture
                "microsoft/DialoGPT-medium", # Fallback
                "gpt2"                       # Final fallback
            ]
            
            for model_name in tokenizer_models:
                try:
                    logger.info(f"ðŸ”„ Trying tokenizer: {model_name}")
                    tokenizer = AutoTokenizer.from_pretrained(model_name)
                    
                    # Configure advanced tokenizer settings
                    if tokenizer.pad_token is None:
                        tokenizer.add_special_tokens({'pad_token': '[PAD]'})
                        tokenizer.pad_token = '[PAD]'  # Ensure PAD exists
                    
                    # Add special tokens for advanced functionality
                    special_tokens = {
                        "additional_special_tokens": [
                            "<|thinking|>", "<|/thinking|>",  # Chain of thought
                            "<|reasoning|>", "<|/reasoning|>", # Reasoning tokens
                            "<|code|>", "<|/code|>",          # Code blocks
                            "<|math|>", "<|/math|>",          # Mathematical content
                            "<|consciousness|>", "<|/consciousness|>", # Consciousness markers
                            "<|memory|>", "<|/memory|>",      # Memory operations
                            "<|expert|>", "<|/expert|>",      # Expert mode
                            "<|multimodal|>", "<|/multimodal|>" # Multimodal content
                        ]
                    }
                    
                    # Add the special tokens
                    num_added = tokenizer.add_special_tokens(special_tokens)
                    logger.info(f"âœ… Added {num_added} special tokens for advanced functionality")
                    
                    self.tokenizer = tokenizer
                    logger.info(f"âœ… Loaded advanced tokenizer: {model_name} (vocab_size: {tokenizer.vocab_size})")
                    break
                    
                except Exception as e:
                    logger.warning(f"Could not load {model_name}: {e}")
                    continue
            else:
                # If all fail, create advanced custom tokenizer
                logger.warning("All pretrained tokenizers failed, creating advanced custom tokenizer")
                self.tokenizer = self._create_advanced_tokenizer()
            
            # Update vocab_size and resize model if needed
            if hasattr(self.tokenizer, 'vocab_size'):
                old_vocab_size = self.vocab_size
                self.vocab_size = self.tokenizer.vocab_size
                # Expected random-guess loss (natural-log). Used for dynamic warning threshold
                self._random_loss_threshold = math.log(self.vocab_size)
                logger.info(f"ðŸ”„ Updated vocab_size to match advanced tokenizer: {self.vocab_size}")
                
                # Demonstrate tokenizer capabilities
                self._demonstrate_tokenizer_capabilities()
                
                # Resize model if already created
                if hasattr(self, 'model') and self.model is not None:
                    self._resize_model_vocab(old_vocab_size, self.vocab_size)
                    
        except Exception as e:
            logger.error(f"Error initializing advanced tokenizer: {e}")
            self.tokenizer = self._create_advanced_tokenizer()

    def _demonstrate_tokenizer_capabilities(self):
        """Demonstrate the advanced tokenizer's capabilities"""
        try:
            test_samples = [
                "The quick brown fox jumps over the lazy dog.",
                "def calculate_fibonacci(n): return n if n <= 1 else calculate_fibonacci(n-1) + calculate_fibonacci(n-2)",
                "The equation E = mcÂ² represents the mass-energy equivalence principle.",
                "I think, therefore I am. This philosophical statement demonstrates reasoning.",
                "import torch; model = torch.nn.Linear(512, 100000)"
            ]
            
            logger.info("ðŸ§ª Testing advanced tokenizer capabilities:")
            for i, sample in enumerate(test_samples):
                tokens = self.tokenizer.encode(sample, max_length=50, truncation=True)
                decoded = self.tokenizer.decode(tokens) if hasattr(self.tokenizer, 'decode') else "decode not available"
                logger.info(f"   Sample {i+1}: {len(tokens)} tokens")
                logger.debug(f"   Original: {sample[:60]}...")
                logger.debug(f"   Tokens: {tokens[:10]}...")
                logger.debug(f"   Decoded: {decoded[:60]}...")
            
            # Test special tokens if available
            if hasattr(self.tokenizer, 'special_tokens'):
                logger.info(f"   Special tokens available: {len(getattr(self.tokenizer, 'special_tokens', {}))}")
                
        except Exception as e:
            logger.warning(f"Could not demonstrate tokenizer capabilities: {e}")
    
    def _resize_model_vocab(self, old_vocab_size: int, new_vocab_size: int):
        """Resize model vocabulary to match tokenizer"""
        try:
            if old_vocab_size == new_vocab_size:
                return
                
            logger.info(f"ðŸ”„ Resizing model vocab from {old_vocab_size} to {new_vocab_size}")
            
            # Resize embedding layer
            if hasattr(self.model, 'embed_tokens'):
                old_embeddings = self.model.embed_tokens
                new_embeddings = nn.Embedding(new_vocab_size, old_embeddings.embedding_dim)
                
                # Copy existing weights
                min_vocab = min(old_vocab_size, new_vocab_size)
                new_embeddings.weight.data[:min_vocab] = old_embeddings.weight.data[:min_vocab]
                
                self.model.embed_tokens = new_embeddings.to(self.device)
                
            # Resize output layer
            if hasattr(self.model, 'lm_head'):
                old_lm_head = self.model.lm_head
                new_lm_head = nn.Linear(old_lm_head.in_features, new_vocab_size, bias=False)
                
                # Copy existing weights
                min_vocab = min(old_vocab_size, new_vocab_size)
                new_lm_head.weight.data[:min_vocab] = old_lm_head.weight.data[:min_vocab]
                
                self.model.lm_head = new_lm_head.to(self.device)
                
            # Update vocab_size attribute in model config if it exists
            if hasattr(self.model, 'config'):
                self.model.config.vocab_size = new_vocab_size
            if hasattr(self.model, 'vocab_size'):
                self.model.vocab_size = new_vocab_size
                
            logger.info(f"âœ… Successfully resized model vocabulary to {new_vocab_size}")
            
        except Exception as e:
            logger.error(f"Error resizing model vocab: {e}")
    
    def _create_advanced_tokenizer(self):
        """Create advanced GPT-4-like tokenizer with BPE and special tokens"""
        logger.info("ðŸ”§ Creating advanced custom tokenizer...")
        
        class AdvancedTokenizer:
            def __init__(self):
                # GPT-4-like vocabulary size
                self.vocab_size = 100000  # Larger vocab for better tokenization
                
                # Special tokens - INITIALIZE FIRST
                self.bos_token = "<|startoftext|>"
                self.eos_token = "<|endoftext|>"
                self.pad_token = "<|pad|>"
                self.unk_token = "<|unknown|>"
                
                # Advanced special tokens
                self.special_tokens = {
                    "<|startoftext|>": 0,
                    "<|endoftext|>": 1,
                    "<|pad|>": 2,
                    "<|unknown|>": 3,
                    "<|thinking|>": 4,
                    "<|/thinking|>": 5,
                    "<|reasoning|>": 6,
                    "<|/reasoning|>": 7,
                    "<|code|>": 8,
                    "<|/code|>": 9,
                    "<|math|>": 10,
                    "<|/math|>": 11,
                    "<|consciousness|>": 12,
                    "<|/consciousness|>": 13,
                    "<|memory|>": 14,
                    "<|/memory|>": 15,
                    "<|expert|>": 16,
                    "<|/expert|>": 17,
                    "<|multimodal|>": 18,
                    "<|/multimodal|>": 19
                }
                
                self.bos_token_id = 0
                self.eos_token_id = 1
                self.pad_token_id = 2
                self.unk_token_id = 3
                
                # Create vocabulary with byte-level encoding (after special_tokens are set)
                self._create_advanced_vocabulary()
                
                logger.info(f"âœ… Created advanced tokenizer with {self.vocab_size} tokens")
            
            def _create_advanced_vocabulary(self):
                """Create sophisticated vocabulary with byte-level BPE"""
                # Start with byte-level tokens (256 bytes)
                self.byte_encoder = {}
                self.byte_decoder = {}
                
                # Create byte-level mappings
                for i in range(256):
                    if i < 32 or i >= 127:  # Non-printable characters
                        char = chr(256 + i)
                    else:
                        char = chr(i)
                    self.byte_encoder[i] = char
                    self.byte_decoder[char] = i
                
                # Common subwords and tokens (simplified BPE)
                common_tokens = [
                    # Single characters
                    *[chr(i) for i in range(32, 127)],
                    # Common subwords
                    "the", "and", "ing", "ion", "tion", "ation", "er", "ed", "ly", "al",
                    "en", "re", "in", "on", "at", "or", "an", "ar", "es", "is", "it",
                    "le", "nd", "st", "te", "to", "ou", "he", "th", "ti", "ve",
                    # Programming tokens
                    "def", "class", "import", "from", "return", "if", "else", "for", "while",
                    "try", "except", "with", "as", "lambda", "yield", "async", "await",
                    # Common words
                    "that", "with", "have", "this", "will", "you", "they", "are", "for",
                    "not", "was", "but", "his", "her", "can", "had", "what", "were",
                    # Numbers
                    *[str(i) for i in range(100)],
                    # Punctuation combinations
                    ".", ",", "!", "?", ":", ";", "'", '"', "(", ")", "[", "]", "{", "}",
                    "...", "!!!", "???", "---", "===", "```", "###"
                ]
                
                # Create token to ID mapping
                self.token_to_id = {}
                self.id_to_token = {}
                
                # Add special tokens first
                current_id = len(self.special_tokens)
                
                # Add common tokens
                for token in common_tokens:
                    if token not in self.token_to_id:
                        self.token_to_id[token] = current_id
                        self.id_to_token[current_id] = token
                        current_id += 1
                
                # Fill remaining vocabulary with generated subwords
                import string
                import itertools
                
                # Generate 2-character combinations
                for c1, c2 in itertools.product(string.ascii_letters + string.digits, repeat=2):
                    if current_id >= self.vocab_size - 1000:  # Leave space for unknown tokens
                        break
                    token = c1 + c2
                    if token not in self.token_to_id:
                        self.token_to_id[token] = current_id
                        self.id_to_token[current_id] = token
                        current_id += 1
                
                # Generate 3-character combinations for common patterns
                common_3char = ["ing", "tion", "ness", "ment", "able", "ible", "ful", "less"]
                for token in common_3char:
                    if current_id >= self.vocab_size - 500:
                        break
                    if token not in self.token_to_id:
                        self.token_to_id[token] = current_id
                        self.id_to_token[current_id] = token
                        current_id += 1
        
            def _tokenize_advanced(self, text):
                """Advanced tokenization with BPE-like splitting"""
                tokens = []
                i = 0
                
                while i < len(text):
                    # Try to match longest possible token
                    matched = False
                    
                    # Try tokens of decreasing length (greedy matching)
                    for length in range(min(10, len(text) - i), 0, -1):
                        substr = text[i:i+length]
                        if substr in self.token_to_id:
                            tokens.append(substr)
                            i += length
                            matched = True
                            break
                    
                    if not matched:
                        # Fall back to character-level or byte-level
                        char = text[i]
                        if char in self.token_to_id:
                            tokens.append(char)
                        else:
                            # Use byte-level encoding for unknown characters
                            byte_tokens = [self.byte_encoder.get(b, self.unk_token) 
                                         for b in char.encode('utf-8')]
                            tokens.extend(byte_tokens)
                        i += 1
                
                return tokens
            
            def encode(self, text, max_length=None, padding=False, truncation=False, return_tensors=None):
                """Advanced encoding with proper BPE tokenization"""
                if isinstance(text, str):
                    # Normalize text
                    text = text.strip()
                    
                    # Tokenize with advanced method
                    tokens = self._tokenize_advanced(text)
                    
                    # Convert to IDs
                    token_ids = []
                    for token in tokens:
                        if token in self.special_tokens:
                            token_ids.append(self.special_tokens[token])
                        elif token in self.token_to_id:
                            token_ids.append(self.token_to_id[token])
                        else:
                            token_ids.append(self.unk_token_id)
                    
                    # Apply truncation
                    if truncation and max_length and len(token_ids) > max_length:
                        token_ids = token_ids[:max_length]
                    
                    # Apply padding
                    if padding and max_length and len(token_ids) < max_length:
                        token_ids.extend([self.pad_token_id] * (max_length - len(token_ids)))
                    
                    return token_ids
                
                return []
            
            def decode(self, token_ids, skip_special_tokens=True):
                """Advanced decoding with proper text reconstruction"""
                import string
                tokens = []
                for token_id in token_ids:
                    if token_id in self.id_to_token:
                        token = self.id_to_token[token_id]
                        if skip_special_tokens and token in self.special_tokens:
                            continue
                        tokens.append(token)
                    elif token_id < len(self.id_to_token):
                        tokens.append(self.id_to_token.get(token_id, self.unk_token))
                
                # Reconstruct text with proper spacing
                text = ""
                for i, token in enumerate(tokens):
                    if i == 0:
                        text = token
                    elif token.startswith(" ") or text.endswith(" "):
                        text += token
                    elif token in string.punctuation:
                        text += token
                    else:
                        text += " " + token
                
                return text.strip()
            
            def __call__(self, text, **kwargs):
                """Make tokenizer callable like HuggingFace tokenizers"""
                token_ids = self.encode(text, **kwargs)
                return {'input_ids': token_ids}
        
        return AdvancedTokenizer()
    
    def _restore_training_state(self) -> None:
        """Restore training state from most advanced checkpoint"""
        try:
            # CRITICAL: Don't reset blockchain training state!
            blockchain_state = self.training_blockchain.get_latest_training_state()
            if blockchain_state:
                logger.info(f"ðŸ”— Blockchain training state exists (epoch {blockchain_state['epoch']}) - skipping regular checkpoint loading")
                return
            
            # Find most recent checkpoint
            latest_checkpoint = self._find_latest_checkpoint()
            
            if latest_checkpoint:
                logger.info(f"ðŸ”„ Restoring training state from {latest_checkpoint}")
                self._load_checkpoint(latest_checkpoint)
            else:
                # Check for distributed checkpoints from peers
                peer_checkpoint = self._find_peer_checkpoint()
                if peer_checkpoint:
                    logger.info(f"ðŸŒ Loading checkpoint from peer network")
                    self._load_peer_checkpoint(peer_checkpoint)
                else:
                    logger.info("ðŸ†• Starting fresh training - no existing checkpoints found")
                    self.current_epoch = 0
            
        except Exception as e:
            logger.error(f"Error restoring training state: {e}")
            # Only reset to 0 if no blockchain state exists
            blockchain_state = self.training_blockchain.get_latest_training_state()
            if not blockchain_state:
                self.current_epoch = 0
    
    def _find_latest_checkpoint(self) -> Optional[str]:
        """Find the most recent checkpoint from BLOCKCHAIN STORAGE ONLY - Single Source of Truth"""
        try:
            # Check blockchain storage ONLY
            blockchain_storage_dir = os.path.join(self.checkpoint_dir, 'training_chain', 'model_storage')
            checkpoint_files = []
            
            if os.path.exists(blockchain_storage_dir):
                checkpoint_files = [f for f in os.listdir(blockchain_storage_dir) 
                                  if f.endswith('.pt') and 'training_epoch_' in f]
            
            if not checkpoint_files:
                logger.info("ðŸ” No blockchain checkpoints found")
                return None
            
            # Sort by epoch number
            def extract_epoch(filename):
                try:
                    if 'training_epoch_' in filename:
                        return int(filename.split('training_epoch_')[1].split('_')[0])
                    else:
                        return 0
                except:
                    return 0
            
            checkpoint_files.sort(key=extract_epoch, reverse=True)
            latest_file = checkpoint_files[0]
            
            # Return the blockchain storage path
            blockchain_path = os.path.join(blockchain_storage_dir, latest_file)
            logger.info(f"ðŸ”— Latest blockchain checkpoint: {blockchain_path}")
            logger.info(f"   ðŸ“ Epoch: {extract_epoch(latest_file)}")
            
            return blockchain_path
            
        except Exception as e:
            logger.error(f"Error finding latest blockchain checkpoint: {e}")
            return None
    
    def _find_peer_checkpoint(self) -> Optional[Dict]:
        """Find most advanced checkpoint from peer network"""
        try:
            if not self.distributed_training_enabled:
                return None
            
            max_epoch = -1
            best_checkpoint_info = None
            
            for node_id, state_info in self.peer_model_states.items():
                if state_info.get('current_epoch', 0) > max_epoch:
                    max_epoch = state_info['current_epoch']
                    best_checkpoint_info = state_info
            
            if max_epoch > self.current_epoch:
                return best_checkpoint_info
            
            return None
            
        except Exception as e:
            logger.error(f"Error finding peer checkpoint: {e}")
            return None
    
    def _load_checkpoint(self, checkpoint_path: str) -> None:
        """Load training state from checkpoint file"""
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # Load model state
            if 'model_state_dict' in checkpoint:
                try:
                    self.model.load_state_dict(checkpoint['model_state_dict'], strict=False)
                    logger.info("âœ… Model state loaded successfully")
                except Exception as e:
                    logger.warning(f"Partial model state loaded: {e}")
            
            # Load optimizer state
            if 'optimizer_state_dict' in checkpoint:
                try:
                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    logger.info("âœ… Optimizer state loaded successfully")
                except Exception as e:
                    logger.warning(f"Could not load optimizer state: {e}")
            
            # Load training metadata
            self.current_epoch = checkpoint.get('epoch', 0)
            self.total_training_steps = checkpoint.get('total_steps', 0)
            self.global_model_version = checkpoint.get('global_version', "1.0.0")
            self.training_history = checkpoint.get('training_history', [])
            self.accumulated_knowledge = checkpoint.get('accumulated_knowledge', {})
            self.consciousness_evolution = checkpoint.get('consciousness_evolution', [])
            self.model_lineage = checkpoint.get('model_lineage', [])
            
            logger.info(f"âœ… Training state restored - Epoch: {self.current_epoch}, "
                       f"Steps: {self.total_training_steps}, Version: {self.global_model_version}")
                       
        except Exception as e:
            logger.error(f"Error loading checkpoint: {e}")
    
    def _load_peer_checkpoint(self, checkpoint_info: Dict) -> None:
        """Load checkpoint from peer network"""
        try:
            # In a real implementation, this would download the checkpoint from the peer
            # For now, we'll simulate updating our state to match the peer
            
            self.current_epoch = checkpoint_info.get('current_epoch', 0)
            self.global_model_version = checkpoint_info.get('global_version', "1.0.0")
            self.total_training_steps = checkpoint_info.get('total_steps', 0)
            
            logger.info(f"ðŸŒ Synchronized with peer checkpoint - Epoch: {self.current_epoch}")
            
        except Exception as e:
            logger.error(f"Error loading peer checkpoint: {e}")
    
    def save_model_checkpoint(self, epoch: int) -> str:
        """Save model checkpoint to IMMUTABLE BLOCKCHAIN ONLY - Single Source of Truth"""
        try:
            # CRITICAL: Ensure epoch only advances, never regresses
            safe_epoch = max(epoch, self.current_epoch)
            if safe_epoch != epoch:
                logger.warning(f"ðŸ›¡ï¸ EPOCH PROTECTION: Adjusted epoch {epoch} â†’ {safe_epoch} (no regression)")
                epoch = safe_epoch
            
            # Update current epoch to new maximum
            self.current_epoch = max(self.current_epoch, epoch)
            
            # Increment global version (progression only)
            version_parts = self.global_model_version.split('.')
            version_parts[2] = str(int(version_parts[2]) + 1)
            self.global_model_version = '.'.join(version_parts)
            
            # Prepare comprehensive training data for blockchain
            model_state = self.model.state_dict()
            optimizer_state = self.optimizer.state_dict()
            
            # Calculate advanced training metrics
            training_metrics = self._calculate_comprehensive_training_metrics(epoch)
            
            # Add additional metadata for comprehensive storage
            training_metrics.update({
                'total_steps': self.total_training_steps,
                'global_version': self.global_model_version,
                'training_history': self.training_history,
                'accumulated_knowledge': self.accumulated_knowledge,
                'consciousness_evolution': self.consciousness_evolution,
                'model_lineage': self.model_lineage,
                'timestamp': time.time(),
                'device': str(self.device),
                'model_type': self.model_type,
                'config': self.config
            })
            
            # Add model-specific metrics (CONVERT TENSORS TO SCALARS!)
            import torch
            if hasattr(self.model, 'consciousness_state'):
                consciousness_state = self.model.consciousness_state
                if torch.is_tensor(consciousness_state):
                    training_metrics['consciousness_state'] = float(consciousness_state.item() if consciousness_state.numel() == 1 else consciousness_state.mean().item())
                else:
                    training_metrics['consciousness_state'] = float(consciousness_state) if consciousness_state is not None else 0.0
            
            if hasattr(self.model, 'quantum_coherence'):
                quantum_coherence = getattr(self.model, 'quantum_coherence', 0.0)
                if torch.is_tensor(quantum_coherence):
                    training_metrics['quantum_coherence'] = float(quantum_coherence.item() if quantum_coherence.numel() == 1 else quantum_coherence.mean().item())
                else:
                    training_metrics['quantum_coherence'] = float(quantum_coherence)
            
            # Generate validation scores (for consensus)
            validation_scores = self._generate_training_validation_scores(training_metrics)
            
            # ðŸ”— ADD TO IMMUTABLE BLOCKCHAIN - SINGLE SOURCE OF TRUTH!
            blockchain_hash = self.training_blockchain.add_training_entry(
                epoch=epoch,
                node_id=self.node_id,
                model_state=model_state,
                optimizer_state=optimizer_state,
                training_metrics=training_metrics,
                validation_scores=validation_scores
            )
            
            # Mine the training into a permanent blockchain block
            mined_block = self.training_blockchain.mine_pending_training(self.node_id)
            
            if mined_block:
                logger.info(f"â›ï¸  BLOCKCHAIN ONLY: Training epoch {epoch} mined into IMMUTABLE blockchain!")
                logger.info(f"   ðŸ”— Block #{mined_block.index} - Hash: {mined_block.block_hash[:16]}...")
                logger.info(f"   ðŸ›¡ï¸ SINGLE SOURCE OF TRUTH: All training data in blockchain storage")
                logger.info(f"   ðŸ“ Path: {mined_block.training_entry.model_storage_path}")
                
                # Update model lineage to reference blockchain storage
                lineage_entry = {
                    'epoch': epoch,
                    'version': self.global_model_version,
                    'timestamp': time.time(),
                    'blockchain_path': mined_block.training_entry.model_storage_path,
                    'block_hash': mined_block.block_hash,
                    'block_index': mined_block.index,
                    'training_loss': self.training_history[-1]['loss'] if self.training_history else 0.0,
                    'blockchain_protected': True,
                    'single_source_truth': True
                }
                self.model_lineage.append(lineage_entry)
                
                # Keep lineage but don't truncate (accumulate knowledge)
                if len(self.model_lineage) > 100:
                    self.model_lineage = self.model_lineage[-100:]
                
                # Update global registry to point to blockchain storage
                self.global_checkpoint_registry[self.global_model_version] = {
                    'epoch': epoch,
                    'blockchain_path': mined_block.training_entry.model_storage_path,
                    'block_hash': mined_block.block_hash,
                    'block_index': mined_block.index,
                    'timestamp': time.time(),
                    'node_id': self.node_id,
                    'size': mined_block.training_entry.checkpoint_size,
                    'blockchain_protected': True,
                    'single_source_truth': True
                }
                
                logger.info(f"ðŸ”— BLOCKCHAIN CHECKPOINT: {mined_block.training_entry.model_storage_path}")
                logger.info(f"ðŸ”„ Global version advanced: {self.global_model_version}")
                logger.info(f"ðŸ›¡ï¸ SINGLE SOURCE: All training state in immutable blockchain storage")
                
                return mined_block.training_entry.model_storage_path
            else:
                logger.error("Failed to mine blockchain block")
                return ""
            
        except Exception as e:
            logger.error(f"Error saving blockchain-only checkpoint: {e}")
            return ""
    
    def load_model_checkpoint(self, checkpoint_path: str) -> bool:
        """Load model checkpoint from BLOCKCHAIN STORAGE - Single Source of Truth"""
        try:
            if not os.path.exists(checkpoint_path):
                logger.error(f"Blockchain checkpoint file not found: {checkpoint_path}")
                return False
            
            logger.info(f"ðŸ”— Loading from BLOCKCHAIN STORAGE: {checkpoint_path}")
            logger.info("ðŸ›¡ï¸ SINGLE SOURCE OF TRUTH: Loading from immutable blockchain checkpoint")
            
            self._load_checkpoint(checkpoint_path)
            
            logger.info("âœ… Blockchain checkpoint loaded successfully")
            return True
            
        except Exception as e:
            logger.error(f"Error loading blockchain checkpoint {checkpoint_path}: {e}")
            return False
    
    def register_peer_model_state(self, node_id: str, model_state_info: Dict) -> None:
        """Register model state from peer node"""
        try:
            self.peer_model_states[node_id] = {
                'current_epoch': model_state_info.get('epoch', 0),
                'global_version': model_state_info.get('version', "1.0.0"),
                'total_steps': model_state_info.get('steps', 0),
                'last_update': time.time(),
                'consciousness_level': model_state_info.get('consciousness', 0.0),
                'model_capabilities': model_state_info.get('capabilities', [])
            }
            
            logger.debug(f"ðŸŒ Registered peer model state from {node_id}")
            
        except Exception as e:
            logger.error(f"Error registering peer model state: {e}")
    
    def get_model_state_summary(self) -> Dict:
        """Get current model state for sharing with peers"""
        try:
            capabilities = []
            consciousness_level = 0.0
            
            if hasattr(self.model, 'consciousness_state'):
                consciousness_level = float(getattr(self.model, 'consciousness_state', 0.0))
                capabilities.append('consciousness')
            
            if hasattr(self.model, 'quantum_coherence'):
                capabilities.append('quantum_processing')
            
            if hasattr(self.model, 'mixture_of_experts'):
                capabilities.append('mixture_of_experts')
            
            if hasattr(self.model, 'self_modify'):
                capabilities.append('self_modification')
            
            return {
                'epoch': self.current_epoch,
                'version': self.global_model_version,
                'steps': self.total_training_steps,
                'consciousness': consciousness_level,
                'capabilities': capabilities,
                'model_type': self.model_type,
                'last_training': self.training_history[-1] if self.training_history else {},
                'lineage_length': len(self.model_lineage)
            }
            
        except Exception as e:
            logger.error(f"Error getting model state summary: {e}")
            return {'epoch': 0, 'version': '1.0.0', 'steps': 0}

    def get_model_state(self):
        """Get model state object for mining and consensus operations"""
        try:
            from .types import ModelState, TrainingMetrics
            
            state_dict = self.model.state_dict()
            state_hash = hashlib.sha256(str(state_dict).encode()).hexdigest()
            weights_hash = hashlib.sha256(str({k: v.sum().item() for k, v in state_dict.items()}).encode()).hexdigest()
            
            consciousness_level = getattr(self.model, 'consciousness_state', 0.0)
            reasoning_quality = getattr(self.model, 'reasoning_quality', 0.0)
            quantum_coherence = getattr(self.model, 'quantum_coherence', 0.0)
            
            last_training = self.training_history[-1] if self.training_history else {}
            current_loss = last_training.get('loss', 0.0)
            
            training_metrics = TrainingMetrics(
                epoch=self.current_epoch,
                loss=current_loss,
                accuracy=0.0,
                perplexity=1.0,
                gradient_norm=0.0,
                samples_processed=self.total_training_steps,
                tokens_processed=self.total_training_steps * self.config.get('batch_size', 1),
                learning_rate=self.config.get('learning_rate', 1e-4),
                consciousness_level=float(consciousness_level),
                reasoning_quality=float(reasoning_quality),
                quantum_coherence=float(quantum_coherence)
            )
            
            return ModelState(
                epoch=self.current_epoch,
                state_hash=state_hash,
                weights_hash=weights_hash,
                training_metrics=training_metrics,
                timestamp=time.time()
            )
            
        except Exception as e:
            logger.error(f"Error getting model state: {e}")
            from .types import ModelState, TrainingMetrics
            return ModelState(
                epoch=self.current_epoch,
                state_hash="error",
                weights_hash="error", 
                training_metrics=TrainingMetrics(
                    epoch=self.current_epoch,
                    loss=0.0,
                    accuracy=0.0,
                    perplexity=1.0,
                    gradient_norm=0.0,
                    samples_processed=0,
                    tokens_processed=0,
                    learning_rate=self.config.get('learning_rate', 1e-4),
                    consciousness_level=0.0,
                    reasoning_quality=0.0,
                    quantum_coherence=0.0
                ),
                timestamp=time.time()
            )
    
    def should_sync_with_peers(self) -> bool:
        """Check if we should sync with peer models"""
        try:
            current_time = time.time()
            
            # Time-based sync
            if current_time - self.last_sync_time > self.model_sync_interval:
                return True
            
            # Check if peers have more advanced models
            for node_id, state_info in self.peer_model_states.items():
                if state_info.get('current_epoch', 0) > self.current_epoch + 2:
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error checking sync requirement: {e}")
            return False
    
    def sync_with_peer_models(self) -> bool:
        """Synchronize with most advanced peer models"""
        try:
            if not self.peer_model_states:
                return False
            
            # Find most advanced peer
            best_peer = None
            max_epoch = self.current_epoch
            
            for node_id, state_info in self.peer_model_states.items():
                if state_info.get('current_epoch', 0) > max_epoch:
                    max_epoch = state_info['current_epoch']
                    best_peer = node_id
            
            if best_peer:
                logger.info(f"ðŸ”„ Syncing with more advanced peer {best_peer} (epoch {max_epoch})")
                
                # In production, this would request and download the model state
                # For now, we'll update our tracking to match the peer
                peer_state = self.peer_model_states[best_peer]
                self.current_epoch = max(self.current_epoch, peer_state.get('current_epoch', 0))
                
                # Update our version to reflect sync
                version_parts = self.global_model_version.split('.')
                version_parts[1] = str(int(version_parts[1]) + 1)
                self.global_model_version = '.'.join(version_parts)
                
                self.last_sync_time = time.time()
                
                logger.info(f"âœ… Synchronized to epoch {self.current_epoch}, version {self.global_model_version}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error syncing with peer models: {e}")
            return False
    
    async def train_epoch(self, requested_epoch: int):
        """Train for one epoch with data lineage tracking and delta-based rewards"""
        try:
            # ðŸ”§ CRITICAL FIX: Always advance to next epoch, never regress
            next_epoch = max(requested_epoch, self.current_epoch + 1)
            if next_epoch != requested_epoch:
                logger.info(f"ðŸ”„ EPOCH ADVANCEMENT: Requested {requested_epoch} â†’ Training {next_epoch}")
            
            epoch = next_epoch
            logger.info(f"ðŸš€ Starting epoch {epoch} with data lineage tracking")
            
            # Get training data with lineage tracking
            training_data = await self.data_engine.acquire_training_data_with_lineage(
                target_samples=self.config.get('samples_per_epoch', 100)
            )
            
            if not training_data:
                logger.warning("No training data available for this epoch")
                return
            
            # Set model to training mode
            self.model.train()
            
            # Initialize epoch metrics
            epoch_loss = 0.0
            batch_count = 0
            processed_samples = 0
            data_deltas = []
            
            # Process data in batches
            batch_size = self.config.get('batch_size', 2)
            for batch_idx in range(0, len(training_data), batch_size):
                batch_data = training_data[batch_idx:batch_idx + batch_size]
                
                # Process each sample in the batch with delta tracking
                batch_loss = 0.0
                for sample in batch_data:
                    # Calculate loss before training on this sample
                    loss_before = self._calculate_sample_loss(sample)
                    
                    # Train on the sample
                    sample_loss = await self._train_on_sample(sample)
                    batch_loss += sample_loss
                    
                    # Calculate loss after training on this sample
                    loss_after = self._calculate_sample_loss(sample)
                    
                    # Record training delta and mark data as consumed
                    if loss_before > loss_after:  # Only record if there's improvement
                        self.data_engine.record_training_improvement(
                            sample, epoch, loss_before, loss_after
                        )
                        data_deltas.append({
                            'sample': sample,
                            'improvement': loss_before - loss_after
                        })
                    
                    # Mark data as consumed
                    self.data_engine.mark_data_consumed_in_training(sample, epoch)
                    processed_samples += 1
                
                # Update model parameters
                gradient_accumulation_steps = self.config.get('gradient_accumulation_steps', 1)
                if batch_count % gradient_accumulation_steps == 0:
                    self.optimizer.step()
                    self.optimizer.zero_grad()
                
                epoch_loss += batch_loss
                batch_count += 1
                
                # Log progress
                if batch_count % 10 == 0:
                    avg_loss = epoch_loss / batch_count
                    logger.info(f"   Batch {batch_count}: avg_loss={avg_loss:.4f}, samples={processed_samples}")
            
            # Final optimizer step
            self.optimizer.step()
            self.optimizer.zero_grad()
            
            # Calculate epoch metrics
            avg_epoch_loss = epoch_loss / batch_count if batch_count > 0 else 0.0
            total_improvement = sum(delta['improvement'] for delta in data_deltas)
            
            # Update training state
            self.current_epoch = epoch
            self.training_loss = avg_epoch_loss
            
            # Calculate and log rewards
            lineage_report = self.data_engine.get_data_lineage_report()
            total_rewards = lineage_report.get('total_rewards', {})
            
            logger.info(f"âœ… Epoch {epoch} completed:")
            logger.info(f"   Average loss: {avg_epoch_loss:.4f}")
            logger.info(f"   Samples processed: {processed_samples}")
            logger.info(f"   Total improvement: {total_improvement:.4f}")
            logger.info(f"   Rewards by source: {total_rewards}")
            
            # Save checkpoint with lineage data
            checkpoint_path = self.save_model_checkpoint(epoch)
            logger.info(f"ðŸ’¾ Checkpoint saved: {checkpoint_path}")
            
            # Update blockchain with training progress
            if hasattr(self, 'blockchain'):
                training_metrics = self._calculate_comprehensive_training_metrics(epoch)
                training_metrics['data_lineage'] = lineage_report
                training_metrics['total_rewards'] = total_rewards
                
                # Add to blockchain
                await self.blockchain.add_training_entry(
                    epoch=epoch,
                    loss=avg_epoch_loss,
                    model_hash=self._calculate_model_hash(),
                    metrics=training_metrics
                )
            
            # ðŸ”§ CRITICAL FIX: Return TrainingProof object instead of dict
            from .types import TrainingProof
            from .crypto import CryptoManager
            
            # Generate hashes for proof
            model_state_hash = self._calculate_model_hash()
            gradient_hash = hashlib.sha256(f"epoch_{epoch}_gradients".encode()).hexdigest()
            dataset_chunk_hash = hashlib.sha256(f"epoch_{epoch}_data".encode()).hexdigest()
            computation_proof = hashlib.sha256(f"epoch_{epoch}_computation".encode()).hexdigest()
            
            # Create training proof object
            training_proof = TrainingProof(
                node_id=self.node_id,
                model_state_hash=model_state_hash,
                gradient_hash=gradient_hash,
                dataset_chunk_hash=dataset_chunk_hash,
                computation_proof=computation_proof,
                timestamp=time.time(),
                signature="",  # Will be signed by consensus
                validation_signatures=[]
            )
            
            return training_proof
            
        except Exception as e:
            logger.error(f"Error in epoch {epoch}: {e}")
            raise
    
    def _calculate_sample_loss(self, sample: str) -> float:
        """Calculate loss for a single sample"""
        try:
            # Tokenize the sample
            inputs = self.tokenizer(
                sample,
                return_tensors='pt',
                max_length=self.max_seq_length,
                truncation=True,
                padding=True
            )
            
            # Move to device
            input_ids = inputs['input_ids'].to(self.device)
            
            # Calculate loss using the model's forward method with labels
            with torch.no_grad():
                # Use input_ids as both input and labels for language modeling
                outputs = self.model(input_ids=input_ids, labels=input_ids)
                
                if isinstance(outputs, dict) and 'loss' in outputs:
                    loss = outputs['loss']
                else:
                    # Fallback to manual calculation
                    logits = outputs.get('logits', outputs) if isinstance(outputs, dict) else outputs
                    # Simple next-token prediction loss
                    shift_logits = logits[..., :-1, :].contiguous()
                    shift_labels = input_ids[..., 1:].contiguous()
                    loss_fct = F.cross_entropy
                    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            
            return float(loss.item()) if isinstance(loss, torch.Tensor) else float(loss)
            
        except Exception as e:
            logger.debug(f"Error calculating sample loss: {e}")
            return 10.0  # Return high loss on error
    
    async def _train_on_sample(self, sample: str) -> float:
        """Train on a single sample and return loss"""
        try:
            # Tokenize the sample
            inputs = self.tokenizer(
                sample,
                return_tensors='pt',
                max_length=self.max_seq_length,
                truncation=True,
                padding=True
            )
            
            # Move to device
            input_ids = inputs['input_ids'].to(self.device)
            
            # Set model to training mode
            self.model.train()
            
            # Forward pass with gradient calculation
            # Use input_ids as both input and labels for language modeling
            outputs = self.model(input_ids=input_ids, labels=input_ids)
            
            if isinstance(outputs, dict) and 'loss' in outputs:
                loss = outputs['loss']
            else:
                # Fallback to manual calculation
                logits = outputs.get('logits', outputs) if isinstance(outputs, dict) else outputs
                shift_logits = logits[..., :-1, :].contiguous()
                shift_labels = input_ids[..., 1:].contiguous()
                loss_fct = torch.nn.CrossEntropyLoss()
                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            
            # Backward pass (accumulate gradients)
            if isinstance(loss, torch.Tensor) and loss.requires_grad:
                loss.backward()
            else:
                logger.warning("Loss tensor does not require gradients - training may not be effective")
            
            return float(loss.item()) if isinstance(loss, torch.Tensor) else float(loss)
            
        except Exception as e:
            logger.debug(f"Error training on sample: {e}")
            return 10.0  # Return high loss on error
    
    def _calculate_model_hash(self) -> str:
        """Calculate hash of current model state"""
        try:
            # Get model state dict
            state_dict = self.model.state_dict()
            
            # Create hash from model parameters
            model_str = ""
            for key, tensor in state_dict.items():
                if tensor.numel() < 1000:  # Only hash small tensors for efficiency
                    model_str += f"{key}:{tensor.sum().item():.6f}"
            
            return hashlib.sha256(model_str.encode()).hexdigest()[:16]
            
        except Exception as e:
            logger.debug(f"Error calculating model hash: {e}")
            return f"epoch_{self.current_epoch}"
    
    def _create_minimal_model(self) -> nn.Module:
        """Create minimal model as fallback"""
        class MinimalModel(nn.Module):
            def __init__(self, vocab_size, embed_dim):
                super().__init__()
                self.embedding = nn.Embedding(vocab_size, embed_dim)
                self.linear = nn.Linear(embed_dim, vocab_size)
                self.consciousness_state = 0.5
                self.quantum_coherence = 50.0
                self.reasoning_quality = 0.0
            
            def forward(self, input_ids):
                x = self.embedding(input_ids)
                return self.linear(x.mean(dim=1, keepdim=True).expand(-1, input_ids.size(1), -1))
        
        actual_vocab_size = getattr(self, 'vocab_size', 1000)
        if hasattr(self, 'tokenizer') and hasattr(self.tokenizer, 'vocab_size'):
            actual_vocab_size = self.tokenizer.vocab_size
        
        logger.info(f"ðŸ”§ Creating minimal fallback model with vocab_size={actual_vocab_size}")
        return MinimalModel(min(actual_vocab_size, 100000), min(self.embed_dim, 256)).to(self.device)
    
    def _initialize_optimizer(self):
        """Initialize optimizer with better settings for stable training"""
        # Use higher learning rate for better convergence
        learning_rate = self.config.get('learning_rate', 1e-4)  # Conservative for stability
        weight_decay = self.config.get('weight_decay', 0.01)
        
        # Clear any existing optimizer state
        if hasattr(self, 'optimizer'):
            del self.optimizer
        
        # Force garbage collection to free memory
        import gc
        gc.collect()
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
        
        # Use AdamW with better parameters
        optimizer = optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay,
            betas=(0.9, 0.95),  # Better betas for language modeling
            eps=1e-8
        )
        
        logger.info(f"ðŸ”§ Optimizer initialized: AdamW(lr={learning_rate}, wd={weight_decay})")
        return optimizer
    
    def get_training_metrics(self) -> Dict[str, Any]:
        """Get comprehensive training metrics"""
        return {
            'current_epoch': self.current_epoch,
            'total_steps': self.total_training_steps,
            'global_version': self.global_model_version,
            'training_history': self.training_history[-10:],  # Last 10 epochs
            'consciousness_evolution': self.consciousness_evolution[-10:],
            'accumulated_knowledge': self.accumulated_knowledge,
            'model_lineage': self.model_lineage,
            'peer_states': self.peer_model_states,
            'last_sync_time': self.last_sync_time
        } 

    def download_and_cache_advanced_models(self) -> List[str]:
        """Download and cache advanced models for offline use"""
        try:
            logger.info("ðŸ“¥ Downloading advanced models for offline use...")
            
            advanced_models = [
                "microsoft/DialoGPT-medium",
                "facebook/opt-350m",
                "EleutherAI/gpt-neo-125M",
                "huggingface/CodeBERTa-small-v1"
            ]
            
            cached_models = []
            cache_dir = os.path.join(os.getcwd(), 'model_cache')
            
            for model_name in advanced_models:
                try:
                    logger.info(f"Downloading {model_name}...")
                    model = self._download_pretrained_model(model_name)
                    if model:
                        cached_models.append(model_name)
                        logger.info(f"âœ… Cached {model_name}")
                    
                except Exception as e:
                    logger.warning(f"Failed to cache {model_name}: {e}")
                    continue
            
            logger.info(f"ðŸ“š Successfully cached {len(cached_models)} advanced models")
            return cached_models
            
        except Exception as e:
            logger.error(f"Error downloading advanced models: {e}")
            return [] 

    def reset_to_base_model(self) -> bool:
        """Reset current model to base - WITH WARNING about losing training progress"""
        try:
            if not os.path.exists(self.base_model_path):
                logger.warning("No base model found to reset to")
                return False
            
            # Detect existing training that would be lost
            existing_progress = self._detect_existing_training_progress()
            if existing_progress:
                logger.warning("ðŸš¨ WARNING: TRAINING PROGRESS WILL BE LOST!")
                logger.warning(f"   Current training at epoch {existing_progress['epoch']}")
                logger.warning(f"   Total steps: {existing_progress.get('total_steps', 0)}")
                logger.warning(f"   Training history: {len(existing_progress.get('training_history', []))} epochs")
                logger.warning("   This operation will reset to base model, losing all progress")
                logger.warning("   Consider using continue training instead")
                
                # Create emergency backup
                timestamp = int(time.time())
                backup_dir = os.path.join(self.checkpoint_dir, f'emergency_backup_{timestamp}')
                os.makedirs(backup_dir, exist_ok=True)
                
                import shutil
                shutil.copy2(existing_progress['checkpoint_path'], 
                           os.path.join(backup_dir, 'last_training_checkpoint.pt'))
                
                logger.info(f"ðŸ†˜ Emergency backup created: {backup_dir}")
            
            logger.info("ðŸ”„ Resetting to base model...")
            self.model = self._load_base_model()
            
            # Reset training state (with warning)
            self.current_epoch = 0
            self.total_training_steps = 0
            self.training_history = []
            self.accumulated_knowledge = {}
            self.consciousness_evolution = []
            
            # Reinitialize optimizer for new model
            self.optimizer = self._initialize_optimizer()
            
            logger.info("âœ… Reset to base model completed")
            if existing_progress:
                logger.warning("âš ï¸  Previous training progress was reset - backup available")
            
            return True
            
        except Exception as e:
            logger.error(f"Error resetting to base model: {e}")
            return False
    
    def force_recreate_base_model(self) -> bool:
        """Force recreation of base model - PROTECTED version that preserves training"""
        try:
            # CRITICAL PROTECTION: Check for existing training first
            existing_progress = self._detect_existing_training_progress()
            if existing_progress:
                logger.warning("ðŸš¨ TRAINING PROTECTION ACTIVE!")
                logger.warning(f"   Existing training found at epoch {existing_progress['epoch']}")
                logger.warning("   CANNOT recreate base model - would lose training progress")
                logger.warning("   Use reset_to_base_model() if you want to restart training")
                return False
            
            logger.info("ðŸ”„ Force recreating base model (SAFE - no existing training detected)...")
            
            # Create backup of existing base model if it exists
            backup_path = None
            if os.path.exists(self.base_model_path):
                backup_path = f"{self.base_model_path}.backup.{int(time.time())}"
                import shutil
                shutil.copy2(self.base_model_path, backup_path)
                logger.info(f"ðŸ“¦ Created backup: {backup_path}")
                
                # Delete original
                os.remove(self.base_model_path)
            
            # Set flags to force download
            old_force_flag = self.force_pretrained_download
            self.force_pretrained_download = True
            
            try:
                # Create new base model
                model = self._create_base_model_with_pretrained_safe()
                self._save_base_model_safe(model)
                
                # Update current model
                self.model = model.to(self.device)
                self.optimizer = self._initialize_optimizer()
                
                logger.info("âœ… Successfully recreated base model (TRAINING-PROTECTED)")
                return True
                
            except Exception as e:
                # Restore backup if creation failed
                if backup_path and os.path.exists(backup_path):
                    import shutil
                    shutil.move(backup_path, self.base_model_path)
                    logger.info("ðŸ”§ Restored backup due to creation failure")
                raise e
            
            finally:
                # Restore flag
                self.force_pretrained_download = old_force_flag
            
        except Exception as e:
            logger.error(f"Error recreating base model (protected): {e}")
            return False 

    def _calculate_comprehensive_training_metrics(self, epoch: int) -> Dict[str, Any]:
        """Calculate comprehensive training metrics for blockchain entry"""
        try:
            # Get recent training metrics
            recent_metrics = self.training_history[-1] if self.training_history else {}
            
            # Calculate consciousness and quantum metrics if available
            consciousness_level = 0.0
            reasoning_quality = 0.0
            quantum_coherence = 0.0
            consciousness_growth = 0.0
            knowledge_accumulation = 0
            expert_utilization = {}
            
            if hasattr(self.model, 'consciousness_state'):
                consciousness_level = float(self.model.consciousness_state.detach().clone().mean().item())
            
            if hasattr(self.model, 'last_reasoning_quality'):
                reasoning_quality = float(self.model.last_reasoning_quality)
            
            if hasattr(self.model, 'quantum_coherence'):
                quantum_coherence = float(getattr(self.model, 'quantum_coherence', 0.0))
            
            # Calculate consciousness growth
            if len(self.consciousness_evolution) > 0:
                last_consciousness = self.consciousness_evolution[-1].get('consciousness_level', 0.0)
                consciousness_growth = consciousness_level - last_consciousness
            
            # Calculate knowledge accumulation
            knowledge_accumulation = len(self.accumulated_knowledge)
            
            # Expert utilization metrics (for MoE models)
            if hasattr(self.model, 'expert_usage_stats'):
                expert_utilization = getattr(self.model, 'expert_usage_stats', {})
            
            return {
                'loss': recent_metrics.get('loss', 0.0),
                'learning_rate': self.optimizer.param_groups[0]['lr'] if self.optimizer else 0.0,
                'batch_count': recent_metrics.get('batch_count', 0),
                'consciousness_level': consciousness_level,
                'reasoning_quality': reasoning_quality,
                'quantum_coherence': quantum_coherence,
                'consciousness_growth': consciousness_growth,
                'knowledge_accumulation': knowledge_accumulation,
                'expert_utilization': expert_utilization,
                'total_steps': self.total_training_steps,
                'global_version': self.global_model_version,
                'timestamp': time.time(),
                'node_id': self.node_id,
                'device': str(self.device)
            }
            
        except Exception as e:
            logger.error(f"Error calculating training metrics: {e}")
            return {
                'loss': 0.0,
                'learning_rate': 0.0,
                'batch_count': 0,
                'consciousness_level': 0.0,
                'reasoning_quality': 0.0,
                'quantum_coherence': 0.0,
                'consciousness_growth': 0.0,
                'knowledge_accumulation': 0,
                'expert_utilization': {},
                'total_steps': self.total_training_steps,
                'timestamp': time.time()
            }
    
    def _generate_training_validation_scores(self, training_metrics: Dict[str, Any]) -> Dict[str, float]:
        """Generate validation scores for consensus verification"""
        try:
            validation_scores = {}
            
            # Loss validation (should be reasonable)
            loss = training_metrics.get('loss', float('inf'))
            if 0.001 <= loss <= 100.0:
                validation_scores['loss_validity'] = 1.0
            elif loss < 0.001:
                validation_scores['loss_validity'] = 0.8  # Suspiciously low
            else:
                validation_scores['loss_validity'] = 0.3  # Too high
            
            # Consciousness validation (should be in reasonable range)
            consciousness = training_metrics.get('consciousness_level', 0.0)
            if 0.0 <= consciousness <= 1.0:
                validation_scores['consciousness_validity'] = 1.0
            else:
                validation_scores['consciousness_validity'] = 0.5
            
            # Quantum coherence validation
            quantum = training_metrics.get('quantum_coherence', 0.0)
            if 0.0 <= quantum <= 100.0:
                validation_scores['quantum_validity'] = 1.0
            else:
                validation_scores['quantum_validity'] = 0.5
            
            # Training progression validation
            if self.training_history:
                last_loss = self.training_history[-1].get('loss', float('inf'))
                current_loss = loss
                
                # Training should generally improve (or stay stable)
                if current_loss <= last_loss * 1.1:  # Allow 10% increase for stability
                    validation_scores['progression_validity'] = 1.0
                else:
                    validation_scores['progression_validity'] = 0.7
            else:
                validation_scores['progression_validity'] = 1.0  # First epoch
            
            # Compute overall validation score
            overall_score = sum(validation_scores.values()) / len(validation_scores)
            validation_scores['overall_validity'] = overall_score
            
            logger.info(f"ðŸ” Training validation score: {overall_score:.3f}")
            
            return validation_scores
            
        except Exception as e:
            logger.error(f"Error generating validation scores: {e}")
            return {'overall_validity': 0.5}  # Neutral score on error
    
    def sync_blockchain_with_peers(self, peer_blockchains: List[Any]) -> bool:
        """Sync training blockchain with peers - only accept more advanced training"""
        try:
            best_sync = False
            
            for peer_blockchain in peer_blockchains:
                if self.training_blockchain.sync_with_peer_chain(peer_blockchain.blocks):
                    best_sync = True
                    logger.info("ðŸ”„ Synced with peer's more advanced training blockchain")
            
            if best_sync:
                # Reload training state from updated blockchain
                self._initialize_from_blockchain()
                
                # Reload model state if needed
                blockchain_state = self.training_blockchain.get_latest_training_state()
                if blockchain_state and blockchain_state.get('model_state_dict'):
                    self.model.load_state_dict(blockchain_state['model_state_dict'], strict=False)
                    logger.info("ðŸ”„ Model state synchronized from blockchain")
            
            return best_sync
            
        except Exception as e:
            logger.error(f"Error syncing blockchain with peers: {e}")
            return False 

    def generate_text(self, prompt: str, max_length: int = 100, temperature: float = 0.7) -> str:
        """Generate text using the trained model"""
        try:
            logger.info(f"ðŸŽ¯ Generating text for prompt: '{prompt[:50]}...'")
            
            # Handle tokenization based on tokenizer type
            if hasattr(self.tokenizer, 'encode') and hasattr(self.tokenizer, 'decode'):
                # Handle our custom tokenizer
                if hasattr(self.tokenizer, '__call__'):
                    # HuggingFace-compatible tokenizer
                    try:
                        inputs = self.tokenizer(prompt, return_tensors='pt', truncation=True, max_length=self.max_seq_length)
                        input_ids = inputs['input_ids'].to(self.device)
                    except Exception as e:
                        logger.warning(f"HuggingFace tokenizer failed: {e}, using encode method")
                        token_ids = self.tokenizer.encode(prompt, max_length=self.max_seq_length, truncation=True)
                        input_ids = torch.tensor([token_ids], dtype=torch.long).to(self.device)
                else:
                    # Custom tokenizer
                    token_ids = self.tokenizer.encode(prompt, max_length=self.max_seq_length, truncation=True)
                    input_ids = torch.tensor([token_ids], dtype=torch.long).to(self.device)
            else:
                logger.error("Tokenizer doesn't have encode/decode methods")
                return ""
            
            logger.info(f"ðŸ”¤ Tokenized input: {input_ids.shape} tokens")
            
            # Generate using the model
            self.model.eval()
            with torch.no_grad():
                if hasattr(self.model, 'generate'):
                    logger.info("ðŸ¤– Using model.generate() method")
                    generated_tokens = self.model.generate(
                        input_ids=input_ids,
                        max_new_tokens=max_length,
                        temperature=temperature,
                        do_sample=temperature > 0,
                        top_p=0.9,
                        pad_token_id=getattr(self.tokenizer, 'pad_token_id', 0),
                        eos_token_id=getattr(self.tokenizer, 'eos_token_id', None)
                    )
                elif hasattr(self.model, 'generate_with_consciousness'):
                    logger.info("ðŸ§  Using model.generate_with_consciousness() method")
                    result = self.model.generate_with_consciousness(
                        input_ids=input_ids,
                        max_new_tokens=max_length,
                        temperature=temperature,
                        return_insights=False
                    )
                    generated_tokens = result.get('generated_tokens', input_ids)
                else:
                    logger.info("ðŸ”§ Using manual generation (model has no generate method)")
                    # Manual generation for models without generate method
                    generated_tokens = input_ids
                    
                    for _ in range(max_length):
                        outputs = self.model(generated_tokens)
                        if isinstance(outputs, dict):
                            logits = outputs.get('logits', outputs.get('last_hidden_state'))
                        elif isinstance(outputs, tuple):
                            logits = outputs[0]
                        else:
                            logits = outputs
                        
                        if logits is None:
                            break
                            
                        # Get next token logits
                        next_token_logits = logits[:, -1, :] / temperature
                        
                        # Apply top-p sampling
                        if temperature > 0:
                            probs = F.softmax(next_token_logits, dim=-1)
                            next_token = torch.multinomial(probs, num_samples=1)
                        else:
                            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                        
                        generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)
                        
                        # Stop if we hit EOS token
                        if hasattr(self.tokenizer, 'eos_token_id') and self.tokenizer.eos_token_id is not None:
                            if next_token.item() == self.tokenizer.eos_token_id:
                                break
            
            logger.info(f"ðŸŽ‰ Generated tokens shape: {generated_tokens.shape}")
            
            # Decode the generated tokens (excluding the original prompt)
            if generated_tokens.shape[1] > input_ids.shape[1]:
                new_tokens = generated_tokens[0][input_ids.shape[1]:]
                logger.info(f"ðŸ”¤ Decoding {len(new_tokens)} new tokens")
                
                # Handle decoding based on tokenizer type
                if hasattr(self.tokenizer, 'decode'):
                    generated_text = self.tokenizer.decode(new_tokens.cpu().tolist(), skip_special_tokens=True)
                else:
                    # Fallback for custom tokenizers
                    generated_text = " ".join([str(token.item()) for token in new_tokens])
                
                logger.info(f"âœ… Generated text: '{generated_text}'")
                return generated_text.strip()
            else:
                logger.warning("âš ï¸ No new tokens generated")
                return ""
                
        except Exception as e:
            logger.error(f"âŒ Error generating text: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return ""

    async def load_training_data(self, samples):
        try:
            if not isinstance(samples, list):
                logger.warning("load_training_data expects a list of strings")
                return False
            # Persist samples using the data engine so they are retrievable via get_training_data
            await self.data_engine._save_training_data(samples)  # noqa
            logger.info(f"Stored {len(samples)} custom training samples for future epochs")
            return True
        except Exception as e:
            logger.error(f"Error storing training data: {e}")
            return False

    def _get_system_memory_info(self):
        """Get system memory information for optimal resource usage"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            total_gb = memory.total / (1024**3)
            available_gb = memory.available / (1024**3)
            used_gb = memory.used / (1024**3)
            
            # Target 90% of total system memory
            target_usage_gb = total_gb * 0.9
            current_usage_percent = (used_gb / total_gb) * 100
            
            return {
                'total_gb': total_gb,
                'available_gb': available_gb,
                'used_gb': used_gb,
                'target_usage_gb': target_usage_gb,
                'current_usage_percent': current_usage_percent,
                'can_use_more': used_gb < target_usage_gb
            }
        except ImportError:
            logger.warning("psutil not available - using conservative memory estimates")
            return {
                'total_gb': 64.0,  # Assume M1 Max specs
                'available_gb': 32.0,
                'used_gb': 16.0,
                'target_usage_gb': 57.6,  # 90% of 64GB
                'current_usage_percent': 25.0,
                'can_use_more': True
            }
    
    def _calculate_optimal_batch_size(self):
        """Calculate optimal batch size based on available memory"""
        memory_info = self._get_system_memory_info()
        
        if torch.backends.mps.is_available():
            try:
                mps_allocated_gb = torch.mps.current_allocated_memory() / (1024**3)
            except:
                mps_allocated_gb = 0
        else:
            mps_allocated_gb = 0
        
        # Base batch size on available memory
        available_for_training = memory_info['target_usage_gb'] - memory_info['used_gb']
        
        if available_for_training > 30:  # Plenty of memory
            return 8
        elif available_for_training > 20:  # Good memory
            return 6
        elif available_for_training > 10:  # Moderate memory
            return 4
        else:  # Conservative
            return 2
    
    def _monitor_and_adjust_resources(self):
        """Monitor resource usage and adjust training parameters dynamically"""
        memory_info = self._get_system_memory_info()
        
        logger.info(f"ðŸŽ¯ System Resource Monitor:")
        logger.info(f"   ðŸ’¾ Total Memory: {memory_info['total_gb']:.1f}GB")
        logger.info(f"   ðŸ“Š Current Usage: {memory_info['used_gb']:.1f}GB ({memory_info['current_usage_percent']:.1f}%)")
        logger.info(f"   ðŸŽ¯ Target Usage: {memory_info['target_usage_gb']:.1f}GB (90%)")
        logger.info(f"   âœ… Can Use More: {memory_info['can_use_more']}")
        
        if torch.backends.mps.is_available():
            try:
                mps_allocated = torch.mps.current_allocated_memory() / (1024**3)
                logger.info(f"   ðŸ”¥ MPS Allocated: {mps_allocated:.1f}GB")
            except:
                pass
        
        # Adjust batch size if needed
        optimal_batch_size = self._calculate_optimal_batch_size()
        current_batch_size = self.config.get('batch_size', 2)
        
        if optimal_batch_size != current_batch_size:
            logger.info(f"ðŸ”§ Adjusting batch size: {current_batch_size} â†’ {optimal_batch_size}")
            self.config['batch_size'] = optimal_batch_size
        
        return memory_info

    def _detect_compute_resources(self):
        """Detect all available compute resources (GPU, CPU, memory) for optimal utilization"""
        resources = {
            'gpus': [],
            'cpu_cores': 0,
            'memory_gb': 0.0,
            'primary_device': 'cpu',
            'can_use_mixed_precision': False,
            'optimal_batch_size': 2,
            'max_sequence_length': 1024
        }
        
        try:
            import psutil
            
            # CPU Detection
            resources['cpu_cores'] = psutil.cpu_count(logical=True)
            physical_cores = psutil.cpu_count(logical=False)
            
            # Memory Detection
            memory = psutil.virtual_memory()
            resources['memory_gb'] = memory.total / (1024**3)
            
            logger.info(f"ðŸ–¥ï¸  CPU Detection:")
            logger.info(f"   Logical Cores: {resources['cpu_cores']}")
            logger.info(f"   Physical Cores: {physical_cores}")
            logger.info(f"   Memory: {resources['memory_gb']:.1f}GB")
            
        except ImportError:
            logger.warning("psutil not available - using conservative estimates")
            resources['cpu_cores'] = 8
            resources['memory_gb'] = 64.0
        
        # GPU Detection - NVIDIA CUDA
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            for i in range(gpu_count):
                props = torch.cuda.get_device_properties(i)
                gpu_memory_gb = props.total_memory / (1024**3)
                gpu_info = {
                    'index': i,
                    'name': props.name,
                    'memory_gb': gpu_memory_gb,
                    'compute_capability': f"{props.major}.{props.minor}",
                    'type': 'cuda'
                }
                resources['gpus'].append(gpu_info)
                logger.info(f"ðŸ”¥ CUDA GPU {i}: {props.name} ({gpu_memory_gb:.1f}GB)")
            
            if gpu_count > 0:
                resources['primary_device'] = 'cuda'
                resources['can_use_mixed_precision'] = True
                # Calculate optimal settings for largest GPU
                max_gpu_memory = max(gpu['memory_gb'] for gpu in resources['gpus'])
                resources['optimal_batch_size'] = min(16, max(4, int(max_gpu_memory // 4)))
                resources['max_sequence_length'] = min(8192, max(2048, int(max_gpu_memory * 256)))
        
        # GPU Detection - Apple Metal (MPS)
        elif torch.backends.mps.is_available():
            # For M1/M2 Macs, memory is unified
            gpu_info = {
                'index': 0,
                'name': 'Apple Metal Performance Shaders',
                'memory_gb': resources['memory_gb'],  # Unified memory
                'compute_capability': 'Metal',
                'type': 'mps'
            }
            resources['gpus'].append(gpu_info)
            resources['primary_device'] = 'mps'
            resources['can_use_mixed_precision'] = True
            
            # Calculate optimal settings for MPS (more conservative due to unified memory)
            usable_memory = resources['memory_gb'] * 0.9  # 90% threshold
            resources['optimal_batch_size'] = min(12, max(4, int(usable_memory // 8)))
            resources['max_sequence_length'] = min(8192, max(2048, int(usable_memory * 128)))
            
            logger.info(f"ðŸŽ Apple Metal GPU: Unified Memory ({resources['memory_gb']:.1f}GB)")
            logger.info(f"   Usable for ML: {usable_memory:.1f}GB (90% threshold)")
        
        # GPU Detection - AMD ROCm (if available)
        elif hasattr(torch.backends, 'cuda') and torch.backends.cuda.is_built():
            try:
                # Check for ROCm
                import subprocess
                result = subprocess.run(['rocm-smi', '--showmeminfo', 'vram'], 
                                      capture_output=True, text=True, timeout=5)
                if result.returncode == 0:
                    logger.info("ðŸ”´ AMD ROCm detected but not fully supported yet")
                    resources['primary_device'] = 'cpu'  # Fallback to CPU for now
            except:
                pass
        
        # Intel GPU Detection (if available)
        elif hasattr(torch, 'xpu') and torch.xpu.is_available():
            try:
                device_count = torch.xpu.device_count()
                for i in range(device_count):
                    gpu_info = {
                        'index': i,
                        'name': 'Intel XPU',
                        'memory_gb': 16.0,  # Conservative estimate
                        'compute_capability': 'XPU',
                        'type': 'xpu'
                    }
                    resources['gpus'].append(gpu_info)
                logger.info(f"âš¡ Intel XPU detected: {device_count} device(s)")
                resources['primary_device'] = 'xpu'
            except:
                pass
        
        # Fallback to CPU optimization
        if not resources['gpus']:
            logger.info("ðŸ”§ No GPU detected - optimizing for CPU training")
            resources['primary_device'] = 'cpu'
            resources['can_use_mixed_precision'] = False
            # CPU-optimized settings
            resources['optimal_batch_size'] = min(4, max(1, resources['cpu_cores'] // 4))
            resources['max_sequence_length'] = min(2048, max(512, int(resources['memory_gb'] * 32)))
        
        # Calculate 90% resource thresholds
        resources['memory_threshold_gb'] = resources['memory_gb'] * 0.9
        resources['cpu_threshold'] = resources['cpu_cores'] * 0.9
        
        if resources['gpus'] and resources['gpus'][0]['type'] != 'mps':
            # For discrete GPUs, use 90% of GPU memory
            resources['gpu_memory_threshold_gb'] = resources['gpus'][0]['memory_gb'] * 0.9
        
        logger.info(f"ðŸŽ¯ Optimal Configuration for 90% Resource Usage:")
        logger.info(f"   Primary Device: {resources['primary_device']}")
        logger.info(f"   Batch Size: {resources['optimal_batch_size']}")
        logger.info(f"   Max Sequence Length: {resources['max_sequence_length']}")
        logger.info(f"   Mixed Precision: {resources['can_use_mixed_precision']}")
        logger.info(f"   Memory Threshold: {resources['memory_threshold_gb']:.1f}GB")
        
        return resources



    def _calculate_device_contribution_capacity(self) -> Dict[str, Any]:
        """Calculate how much this device can contribute to training the same global model"""
        memory_gb = self.compute_resources.get('memory_threshold_gb', 8.0)
        cpu_cores = self.compute_resources.get('cpu_cores', 4)
        has_gpu = self.compute_resources.get('gpu_available', False)
        
        # Same model architecture for everyone - only contribution capacity varies
        base_model_config = {
            'hidden_size': 768,        # Fixed model architecture
            'num_layers': 12,          # Same for all devices
            'num_heads': 12,           # Global consensus model
            'intermediate_size': 3072,  # Consistent across network
            'vocab_size': 50257        # Standard tokenizer
        }
        
        # Adaptive contribution based on hardware (like Bitcoin mining hashrate)
        if memory_gb >= 48:  # High-end: Can process large batches fast
            contribution = {
                'batch_size': 16,
                'max_seq_length': 2048,
                'gradient_accumulation': 2,
                'contribution_tier': 'whale',      # Like Bitcoin whales
                'expected_blocks_per_hour': 12,
                'max_batches_per_epoch': 1000
            }
        elif memory_gb >= 16:  # Mid-range: Good contribution
            contribution = {
                'batch_size': 8,
                'max_seq_length': 1024,
                'gradient_accumulation': 4,
                'contribution_tier': 'miner',      # Regular miners
                'expected_blocks_per_hour': 6,
                'max_batches_per_epoch': 500
            }
        elif memory_gb >= 8:  # Basic: Still valuable
            contribution = {
                'batch_size': 4,
                'max_seq_length': 512,
                'gradient_accumulation': 8,
                'contribution_tier': 'participant', # Small participants
                'expected_blocks_per_hour': 2,
                'max_batches_per_epoch': 200
            }
        else:  # Mobile: Minimal but still counts
            contribution = {
                'batch_size': 2,
                'max_seq_length': 256,
                'gradient_accumulation': 16,
                'contribution_tier': 'mobile',     # Mobile mining
                'expected_blocks_per_hour': 1
            }
        
        # GPU bonus (like ASIC miners vs CPU miners)
        if has_gpu:
            contribution['batch_size'] = min(contribution['batch_size'] * 2, 32)
            contribution['expected_blocks_per_hour'] *= 2
            contribution['gpu_accelerated'] = True
        else:
            contribution['gpu_accelerated'] = False
        
        # Multi-core scaling
        core_multiplier = min(cpu_cores / 4.0, 2.0)  # Cap at 2x boost
        contribution['batch_size'] = int(contribution['batch_size'] * core_multiplier)
        contribution['expected_blocks_per_hour'] = int(contribution['expected_blocks_per_hour'] * core_multiplier)
        
        logger.info(f"ðŸ­ Device Mining Capacity Analysis:")
        logger.info(f"   ðŸ“Š Contribution Tier: {contribution['contribution_tier']}")
        logger.info(f"   âš¡ Expected Training Blocks/Hour: {contribution['expected_blocks_per_hour']}")
        logger.info(f"   ðŸ”¥ Batch Processing: {contribution['batch_size']} samples")
        logger.info(f"   ðŸ“ Sequence Length: {contribution['max_seq_length']} tokens")
        logger.info(f"   ðŸŽ¯ GPU Accelerated: {contribution['gpu_accelerated']}")
        logger.info(f"   ðŸŒ Training SAME global model as entire network")
        
        return {**base_model_config, **contribution}

    async def initialize_training_with_pretrained_data(self):
        """Initialize training by extracting and registering pretrained model data"""
        try:
            logger.info("ðŸŽ¯ Initializing training with pretrained model data as first step")
            
            # Check if we already have pretrained data registered
            lineage_report = self.data_engine.get_data_lineage_report()
            pretrained_stats = lineage_report.get('source_statistics', {}).get('pretrained_model', {})
            
            if pretrained_stats.get('total', 0) > 0:
                logger.info(f"âœ… Already have {pretrained_stats['total']} pretrained data entries")
                unconsumed = pretrained_stats['total'] - pretrained_stats.get('consumed', 0)
                logger.info(f"   Unconsumed pretrained data: {unconsumed}")
                return
            
            # Get the best pretrained model for data extraction
            best_model_info = self._select_best_pretrained_model()
            if not best_model_info:
                logger.warning("No suitable pretrained model found for data extraction")
                return
            
            model_name = best_model_info['name']
            logger.info(f"ðŸ”„ Downloading {model_name} for data extraction...")
            
            # Download the pretrained model
            pretrained_model = self._download_pretrained_model_advanced(model_name)
            if not pretrained_model:
                logger.warning(f"Failed to download {model_name}")
                return
            
            # Extract training data from the pretrained model
            logger.info(f"ðŸ“š Extracting training data from {model_name}...")
            extracted_data = await self.data_engine.pretrained_extractor.extract_from_pretrained_model(
                model_name, pretrained_model
            )
            
            if extracted_data:
                logger.info(f"âœ… Successfully extracted {len(extracted_data)} training samples")
                logger.info(f"   Source: {model_name}")
                logger.info(f"   Quality: High (0.9)")
                logger.info(f"   Ready for training with reward tracking")
            else:
                logger.warning("No data extracted from pretrained model")
            
            # Also extract weights if configured
            if self.load_pretrained_base:
                logger.info("ðŸ”„ Applying pretrained weights to model...")
                success = self._apply_pretrained_weights_advanced(self.model, pretrained_model, best_model_info)
                if success:
                    logger.info("âœ… Pretrained weights applied successfully")
                    
                    # Register the weight initialization as a training delta
                    initial_improvement = 2.0  # Assume pretrained weights provide significant improvement
                    self.data_engine.lineage_manager.record_training_delta(
                        epoch=0,
                        data_hash=f"pretrained_weights_{model_name}",
                        loss_before=10.0,  # Assume high initial loss
                        loss_after=8.0,   # Assume improvement from pretrained weights
                        data_contribution=1.0
                    )
            
            # Update statistics
            final_report = self.data_engine.get_data_lineage_report()
            logger.info("ðŸ“Š Pretrained data initialization complete:")
            logger.info(f"   Total data entries: {final_report['total_data_entries']}")
            logger.info(f"   Pretrained model entries: {final_report['source_statistics'].get('pretrained_model', {}).get('total', 0)}")
            logger.info(f"   Ready for delta-based training")
            
        except Exception as e:
            logger.error(f"Error initializing with pretrained data: {e}")
    
    def get_training_readiness_report(self) -> Dict[str, Any]:
        """Get comprehensive report on training readiness"""
        try:
            lineage_report = self.data_engine.get_data_lineage_report()
            
            # Calculate data availability by source
            source_readiness = {}
            for source, stats in lineage_report.get('source_statistics', {}).items():
                total = stats.get('total', 0)
                consumed = stats.get('consumed', 0)
                unconsumed = total - consumed
                
                source_readiness[source] = {
                    'total_data': total,
                    'consumed_data': consumed,
                    'available_data': unconsumed,
                    'quality_avg': stats.get('quality_avg', 0.0),
                    'readiness_score': min(1.0, unconsumed / 50.0)  # Assume 50 samples is good readiness
                }
            
            # Calculate overall readiness
            total_available = sum(s['available_data'] for s in source_readiness.values())
            overall_readiness = min(1.0, total_available / 200.0)  # Assume 200 samples is fully ready
            
            # Check model state
            model_state = {
                'current_epoch': self.current_epoch,
                'has_pretrained_weights': self.load_pretrained_base,
                'model_type': self.model_type,
                'vocabulary_size': self.vocab_size,
                'device': str(self.device)
            }
            
            return {
                'overall_readiness': overall_readiness,
                'total_available_data': total_available,
                'source_readiness': source_readiness,
                'model_state': model_state,
                'reward_system': {
                    'total_rewards': lineage_report.get('total_rewards', {}),
                    'training_deltas': lineage_report.get('training_deltas', 0)
                },
                'recommendations': self._generate_training_recommendations(source_readiness, overall_readiness)
            }
            
        except Exception as e:
            logger.error(f"Error generating readiness report: {e}")
            return {'error': str(e)}
    
    def _generate_training_recommendations(self, source_readiness: Dict, overall_readiness: float) -> List[str]:
        """Generate training recommendations based on data readiness"""
        recommendations = []
        
        if overall_readiness < 0.3:
            recommendations.append("âš ï¸ Low data availability - consider acquiring more training data")
        
        # Check pretrained model data
        pretrained_readiness = source_readiness.get('pretrained_model', {})
        if pretrained_readiness.get('available_data', 0) == 0:
            recommendations.append("ðŸŽ¯ Initialize with pretrained model data for better starting point")
        
        # Check data source diversity
        available_sources = sum(1 for s in source_readiness.values() if s['available_data'] > 0)
        if available_sources < 2:
            recommendations.append("ðŸŒ Diversify data sources for better training outcomes")
        
        # Check quality
        avg_quality = sum(s['quality_avg'] for s in source_readiness.values()) / len(source_readiness) if source_readiness else 0
        if avg_quality < 0.7:
            recommendations.append("ðŸ“ˆ Focus on higher quality data sources")
        
        if overall_readiness > 0.8:
            recommendations.append("âœ… Training data is ready - begin training for optimal rewards")
        
        return recommendations

    def _initialize_optimized_random_weights(self, model: nn.Module) -> None:
        """Initialize model with optimized random weights"""
        try:
            logger.info("ðŸŽ² Initializing model with optimized random weights")
            
            # Apply Xavier/Glorot initialization for better convergence
            for name, param in model.named_parameters():
                if 'weight' in name:
                    if len(param.shape) >= 2:
                        # Use Xavier initialization for linear layers
                        nn.init.xavier_uniform_(param)
                    else:
                        # Use normal initialization for 1D parameters
                        nn.init.normal_(param, mean=0.0, std=0.02)
                elif 'bias' in name:
                    # Initialize biases to zero
                    nn.init.zeros_(param)
            
            logger.info("âœ… Optimized random weights initialized")
            
        except Exception as e:
            logger.error(f"Error initializing optimized random weights: {e}")
    
    def _transfer_transformer_layers(self, pretrained_model: Any, target_model: nn.Module) -> int:
        """Transfer transformer layers from pretrained to target model"""
        try:
            transferred_layers = 0
            
            # This is a simplified version - in production, you'd implement
            # sophisticated layer mapping based on model architectures
            pretrained_state = pretrained_model.state_dict()
            target_state = target_model.state_dict()
            
            # Look for transformer layer patterns
            for target_key in target_state.keys():
                if 'transformer' in target_key or 'layer' in target_key:
                    # Try to find corresponding layer in pretrained model
                    for pretrained_key in pretrained_state.keys():
                        if self._keys_are_compatible(target_key, pretrained_key):
                            pretrained_weight = pretrained_state[pretrained_key]
                            target_weight = target_state[target_key]
                            
                            if pretrained_weight.shape == target_weight.shape:
                                target_state[target_key] = pretrained_weight.clone()
                                transferred_layers += 1
                                break
            
            # Load the updated state
            if transferred_layers > 0:
                target_model.load_state_dict(target_state, strict=False)
            
            return transferred_layers
            
        except Exception as e:
            logger.debug(f"Error transferring transformer layers: {e}")
            return 0
    
    def _keys_are_compatible(self, target_key: str, pretrained_key: str) -> bool:
        """Check if two parameter keys are compatible for transfer"""
        # Simple compatibility check - can be made more sophisticated
        target_parts = target_key.split('.')
        pretrained_parts = pretrained_key.split('.')
        
        # Check if they have similar structure
        if len(target_parts) != len(pretrained_parts):
            return False
        
        # Check for similar naming patterns
        for t_part, p_part in zip(target_parts, pretrained_parts):
            if t_part == p_part:
                continue
            elif 'weight' in t_part and 'weight' in p_part:
                continue
            elif 'bias' in t_part and 'bias' in p_part:
                continue
            else:
                return False
        
        return True
    
    def _transfer_embedding_layers(self, pretrained_state: Dict, target_state: Dict) -> int:
        """Transfer embedding layers from pretrained to target model"""
        try:
            transferred_layers = 0
            
            # Common embedding layer names
            embedding_patterns = [
                'embeddings.word_embeddings.weight',
                'embeddings.position_embeddings.weight',
                'wte.weight',  # GPT-style
                'wpe.weight',  # GPT-style
                'embed_tokens.weight',  # Other models
                'embed_positions.weight'
            ]
            
            for pattern in embedding_patterns:
                if pattern in pretrained_state and pattern in target_state:
                    pretrained_weight = pretrained_state[pattern]
                    target_weight = target_state[pattern]
                    
                    # Try to adapt the embedding if shapes don't match
                    if pretrained_weight.shape == target_weight.shape:
                        target_state[pattern] = pretrained_weight.clone()
                        transferred_layers += 1
                    else:
                        # Try to adapt embedding size
                        adapted_weight = self._adapt_embedding_size(pretrained_weight, target_weight.shape)
                        if adapted_weight is not None:
                            target_state[pattern] = adapted_weight
                            transferred_layers += 1
            
            return transferred_layers
            
        except Exception as e:
            logger.debug(f"Error transferring embedding layers: {e}")
            return 0
    
    def _adapt_embedding_size(self, pretrained_embedding: torch.Tensor, target_shape: torch.Size) -> Optional[torch.Tensor]:
        """Adapt embedding size to match target shape"""
        try:
            if len(target_shape) != 2 or len(pretrained_embedding.shape) != 2:
                return None
            
            target_vocab_size, target_embed_dim = target_shape
            pretrained_vocab_size, pretrained_embed_dim = pretrained_embedding.shape
            
            # Create new embedding tensor
            adapted_embedding = torch.zeros(target_shape, dtype=pretrained_embedding.dtype)
            
            # Copy overlapping vocabulary
            min_vocab = min(target_vocab_size, pretrained_vocab_size)
            min_embed = min(target_embed_dim, pretrained_embed_dim)
            
            adapted_embedding[:min_vocab, :min_embed] = pretrained_embedding[:min_vocab, :min_embed]
            
            # Initialize new vocabulary entries with small random values
            if target_vocab_size > pretrained_vocab_size:
                nn.init.normal_(adapted_embedding[pretrained_vocab_size:, :min_embed], mean=0.0, std=0.02)
            
            # Initialize new embedding dimensions with small random values
            if target_embed_dim > pretrained_embed_dim:
                nn.init.normal_(adapted_embedding[:min_vocab, pretrained_embed_dim:], mean=0.0, std=0.02)
            
            return adapted_embedding
            
        except Exception as e:
            logger.debug(f"Error adapting embedding size: {e}")
            return None

    def verify_single_source_of_truth(self) -> Dict[str, Any]:
        """Verify blockchain storage is the single source of truth and clean up redundant files"""
        try:
            report = {
                'blockchain_files': 0,
                'redundant_files_found': 0,
                'redundant_files_cleaned': 0,
                'single_source_verified': False,
                'latest_blockchain_epoch': 0,
                'blockchain_storage_path': "",
                'errors': []
            }
            
            # Check blockchain storage
            blockchain_storage_dir = os.path.join(self.checkpoint_dir, 'training_chain', 'model_storage')
            if os.path.exists(blockchain_storage_dir):
                blockchain_files = [f for f in os.listdir(blockchain_storage_dir) 
                                  if f.endswith('.pt') and 'training_epoch_' in f]
                report['blockchain_files'] = len(blockchain_files)
                report['blockchain_storage_path'] = blockchain_storage_dir
                
                if blockchain_files:
                    def extract_epoch(filename):
                        try:
                            return int(filename.split('training_epoch_')[1].split('_')[0])
                        except:
                            return 0
                    
                    epochs = [extract_epoch(f) for f in blockchain_files]
                    report['latest_blockchain_epoch'] = max(epochs) if epochs else 0
            
            # Check for redundant traditional checkpoint files
            if os.path.exists(self.checkpoint_dir):
                traditional_files = [f for f in os.listdir(self.checkpoint_dir) 
                                   if f.endswith('.pt') and ('model_epoch_' in f or ('epoch_' in f and f != 'base_model.pt'))]
                report['redundant_files_found'] = len(traditional_files)
                
                # Offer to clean up redundant files
                if traditional_files:
                    logger.warning(f"ðŸ§¹ CLEANUP: Found {len(traditional_files)} redundant traditional checkpoint files")
                    logger.warning("   These files are no longer needed with blockchain-only storage")
                    
                    cleaned_count = 0
                    for filename in traditional_files:
                        try:
                            filepath = os.path.join(self.checkpoint_dir, filename)
                            
                            # Create backup first
                            backup_dir = os.path.join(self.checkpoint_dir, 'traditional_backup')
                            os.makedirs(backup_dir, exist_ok=True)
                            backup_path = os.path.join(backup_dir, filename)
                            
                            import shutil
                            shutil.move(filepath, backup_path)
                            cleaned_count += 1
                            logger.info(f"   ðŸ“¦ Moved to backup: {filename}")
                            
                        except Exception as e:
                            report['errors'].append(f"Failed to backup {filename}: {e}")
                    
                    report['redundant_files_cleaned'] = cleaned_count
                    
                    if cleaned_count > 0:
                        logger.info(f"âœ… CLEANUP COMPLETE: {cleaned_count} files moved to backup")
                        logger.info(f"   ðŸ“ Backup location: {backup_dir}")
            
            # Verify single source of truth
            if report['blockchain_files'] > 0 and report['redundant_files_found'] == 0:
                report['single_source_verified'] = True
                logger.info("ðŸ›¡ï¸ SINGLE SOURCE VERIFIED: Blockchain storage is the only checkpoint source")
                logger.info(f"   ðŸ“Š Blockchain checkpoints: {report['blockchain_files']}")
                logger.info(f"   ðŸ”¢ Latest epoch: {report['latest_blockchain_epoch']}")
            elif report['blockchain_files'] > 0:
                logger.info("ðŸ”„ SINGLE SOURCE PROGRESS: Blockchain storage active, redundant files cleaned")
                logger.info(f"   ðŸ“Š Blockchain checkpoints: {report['blockchain_files']}")
                logger.info(f"   ðŸ”¢ Latest epoch: {report['latest_blockchain_epoch']}")
            else:
                logger.warning("âš ï¸ No blockchain checkpoints found - starting fresh training")
            
            return report
            
        except Exception as e:
            logger.error(f"Error verifying single source of truth: {e}")
            return {'single_source_verified': False, 'errors': [str(e)]}

    def get_blockchain_storage_info(self) -> Dict[str, Any]:
        """Get comprehensive information about blockchain storage state"""
        try:
            info = {
                'blockchain_enabled': True,
                'storage_directory': os.path.join(self.checkpoint_dir, 'training_chain', 'model_storage'),
                'checkpoint_count': 0,
                'total_size_bytes': 0,
                'epoch_range': {'min': 0, 'max': 0},
                'latest_checkpoint': None,
                'blockchain_integrity': {'verified': False, 'errors': []},
                'single_source_active': False
            }
            
            blockchain_storage_dir = info['storage_directory']
            
            if os.path.exists(blockchain_storage_dir):
                checkpoint_files = [f for f in os.listdir(blockchain_storage_dir) 
                                  if f.endswith('.pt') and 'training_epoch_' in f]
                info['checkpoint_count'] = len(checkpoint_files)
                
                if checkpoint_files:
                    # Calculate total size
                    total_size = 0
                    epochs = []
                    
                    for filename in checkpoint_files:
                        filepath = os.path.join(blockchain_storage_dir, filename)
                        if os.path.exists(filepath):
                            total_size += os.path.getsize(filepath)
                            
                            # Extract epoch
                            try:
                                epoch = int(filename.split('training_epoch_')[1].split('_')[0])
                                epochs.append(epoch)
                            except:
                                pass
                    
                    info['total_size_bytes'] = total_size
                    
                    if epochs:
                        info['epoch_range']['min'] = min(epochs)
                        info['epoch_range']['max'] = max(epochs)
                        
                        # Find latest checkpoint
                        latest_epoch = max(epochs)
                        latest_files = [f for f in checkpoint_files if f'training_epoch_{latest_epoch}_' in f]
                        if latest_files:
                            info['latest_checkpoint'] = os.path.join(blockchain_storage_dir, latest_files[-1])
            
            # Check blockchain integrity
            if hasattr(self, 'training_blockchain'):
                is_valid, errors = self.training_blockchain.verify_training_chain()
                info['blockchain_integrity']['verified'] = is_valid
                info['blockchain_integrity']['errors'] = errors
            
            # Check if single source is active (no redundant files)
            redundant_files = []
            if os.path.exists(self.checkpoint_dir):
                redundant_files = [f for f in os.listdir(self.checkpoint_dir) 
                                 if f.endswith('.pt') and ('model_epoch_' in f or ('epoch_' in f and f != 'base_model.pt'))]
            
            info['single_source_active'] = len(redundant_files) == 0 and info['checkpoint_count'] > 0
            
            return info
            
        except Exception as e:
            logger.error(f"Error getting blockchain storage info: {e}")
            return {'blockchain_enabled': False, 'error': str(e)}
